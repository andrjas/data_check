{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to data_check data_check is a simple data validation tool. In its most basic form, it will execute SQL queries and compare the results against CSV or Excel files. But there are more advanced features: Features CSV checks : compare SQL queries against CSV files Excel support: Use Excel (xlsx) instead of CSV multiple environments (databases) in the configuration file populate tables from CSV or Excel files execute any SQL files on a database more complex pipelines run any script/command (via pipelines) simplified checks for empty datasets lookups to reuse the same data in multiple queries Database support data_check should work with any database that works with SQLAlchemy . Currently data_check is tested against PostgreSQL, MySQL, SQLite, Oracle and Microsoft SQL Server. Why? data_check tries to solve a simple problem in data domains: during development you create multiple SQL queries to validate your data and compare the result manually. With data_check you can organize your data tests and run them automatically, for example in a CI pipeline. How to get started First install data_check. You can then try data_check with the sample project . To create a new project folder write a data_check.yml for the configuration and put your test files in the checks folder. Project layout data_check has a simple layout for projects: a single configuration file and a folder with the test files. You can also organize the test files in subfolders. data_check.yml # The configuration file checks/ # Default folder for data tests some_test.sql # SQL file with the query to run against the database some_test.csv # CSV file with the expected result other_test.sql # SQL file with another test other_test.xlsx # Expected result for other_test.sql in an Excel file empty_result.sql # SQL file with a result set that is expected to be empty empty_result.empty # empty file for empty_result.sql subfolder/ # Tests can be nested in subfolders Configuration data_check uses the data_check.yml file in the current folder for configuration. This is a simple YAML file: default_connection: con connections: con: connection-string Under connections you can put multiple connection strings with names. default_connection is the connection name that data_check uses when no additional arguments are given. You can also use environment variables in the connection string to store the credentials outside data_check.yml (e.g. postgresql://postgres:${POSTGRES_PWD}@db:5432 ). Test files The test files are organized in the checks folder and its subfolders. data_check will run the queries for each SQL file in these folders and compare the results with the corresponding CSV files. The CSV file must be named exactly like the SQL file, only with different file endings. Instead of writing the CSV files manually, you can also generate them from the SQL files. How it works data_check uses pandas and SQLAlchemy internally. The SQL files are executed and converted to DataFrames. The CSV files are also parsed to DataFrames. Both DataFrames are then merged with a full outer join. If some rows differ, the test is considered as failed.","title":"Home"},{"location":"#welcome-to-data_check","text":"data_check is a simple data validation tool. In its most basic form, it will execute SQL queries and compare the results against CSV or Excel files. But there are more advanced features:","title":"Welcome to data_check"},{"location":"#features","text":"CSV checks : compare SQL queries against CSV files Excel support: Use Excel (xlsx) instead of CSV multiple environments (databases) in the configuration file populate tables from CSV or Excel files execute any SQL files on a database more complex pipelines run any script/command (via pipelines) simplified checks for empty datasets lookups to reuse the same data in multiple queries","title":"Features"},{"location":"#database-support","text":"data_check should work with any database that works with SQLAlchemy . Currently data_check is tested against PostgreSQL, MySQL, SQLite, Oracle and Microsoft SQL Server.","title":"Database support"},{"location":"#why","text":"data_check tries to solve a simple problem in data domains: during development you create multiple SQL queries to validate your data and compare the result manually. With data_check you can organize your data tests and run them automatically, for example in a CI pipeline.","title":"Why?"},{"location":"#how-to-get-started","text":"First install data_check. You can then try data_check with the sample project . To create a new project folder write a data_check.yml for the configuration and put your test files in the checks folder.","title":"How to get started"},{"location":"#project-layout","text":"data_check has a simple layout for projects: a single configuration file and a folder with the test files. You can also organize the test files in subfolders. data_check.yml # The configuration file checks/ # Default folder for data tests some_test.sql # SQL file with the query to run against the database some_test.csv # CSV file with the expected result other_test.sql # SQL file with another test other_test.xlsx # Expected result for other_test.sql in an Excel file empty_result.sql # SQL file with a result set that is expected to be empty empty_result.empty # empty file for empty_result.sql subfolder/ # Tests can be nested in subfolders","title":"Project layout"},{"location":"#configuration","text":"data_check uses the data_check.yml file in the current folder for configuration. This is a simple YAML file: default_connection: con connections: con: connection-string Under connections you can put multiple connection strings with names. default_connection is the connection name that data_check uses when no additional arguments are given. You can also use environment variables in the connection string to store the credentials outside data_check.yml (e.g. postgresql://postgres:${POSTGRES_PWD}@db:5432 ).","title":"Configuration"},{"location":"#test-files","text":"The test files are organized in the checks folder and its subfolders. data_check will run the queries for each SQL file in these folders and compare the results with the corresponding CSV files. The CSV file must be named exactly like the SQL file, only with different file endings. Instead of writing the CSV files manually, you can also generate them from the SQL files.","title":"Test files"},{"location":"#how-it-works","text":"data_check uses pandas and SQLAlchemy internally. The SQL files are executed and converted to DataFrames. The CSV files are also parsed to DataFrames. Both DataFrames are then merged with a full outer join. If some rows differ, the test is considered as failed.","title":"How it works"},{"location":"csv_checks/","text":"CSV checks This is the default mode when running data_check. data_check expects a SQL file and a CSV file . The SQL file will be executed against the database and the result is compared with the CSV file. If they match, the test is passed, otherwise it fails. SQL file Each SQL file must contain a single SQL query. The query will be executed in the database, hence you must use the SQL dialect that the database in use understands. Templates SQL files can contain Jinja2 templates. The templates are replaced with the values defined in checks/template.yml . Example: SQL file: select '{{template_value}}' as test checks/template.yml : template_value: ABC CSV file: test ABC CSV format data_check uses a basic CSV format. Each column is separated by a comma without any space around them. The first line must contain a header. The columns must match the columns in the SQL file. Any columns that do not match between the CSV and the SQL file will be ignored. string_header,int_header,float_header,date_header,null_header string,42,42.1,2020-12-20, # a comment line \"second row\",42,42.1,2020-12-20, Anything after '#' is regarded as a comment. You can use comments to annotate the date as they will be completely ignored. You can escape the start of the comment with '\\' and threat the rest of the line as data. Only the data types strings, decimals and date/timestamps (partially) are supported. Strings can be optionally enclosed in double quotes (\"). Empty strings are treated as null values. data_check recognizes if a column in a SQL query is a date or timestamp and converts the columns in the CSV automatically to timestamps. For some databases (PostgreSQL, MySQL) this only works for timestamp/datetime columns, not date columns. The date format in the CSV file must use ISO 8601 , for example \"2021-12-30\" or \"2021-12-30 13:45:56\". Any other data type must be converted to strings/varchar in the SQL query. Empty dataset checks If you expect the result of the SQL query to be empty, you do not have to write a CSV file with the header. Instead, create a file with the ending .empty . Since the column names do not matter, the check will pass, if the SQL query does not return any values. The .empty file can be empty or contain any data. data_check does not read the content from this file. Example: empty_query.sql empty_query.empty Generating expectation files If you run data_check --generate in a project folder, data_check will execute the query for each SQL file where the CSV file is missing and write the results into the CSV file. You can add --force to overwrite existing CSV files. You can also generate expectation files for pipelines . If you run --generate on a project with pipelines, beware though that the pipelines will be executed! Lookups Lookups are a way to reuse the result of a query in multiple other queries. You can use lookups for example to return all table names that you want to exclude from your tests and use the table names to filter them from your queries. To create a lookup, put a SQL file in the lookups folder. To use the lookup in a query, use the lookup name like a subquery or list. The lookup name is the SQL filename prefixed with a colon (':') and without the file ending. If a lookup file is in subfolders, each subfolder is part of the lookup name with a double underscore ('__') as the separator between folders and the SQL filename. Only the first column from a lookup query is used, other columns are ignored. Example: some_check.sql : select a from some_table where b in :b1 or a in :sub_lkp__b2 lookups/b1.sql : select 'b' as b union all select 'c' as b lookups/sub_lkp/b2.sql : select 1 as b union all select 2 as b In this example :b1 is loaded from the file lookups/b1.sql and :sub_lkp__b2 from lookups/sub_lkp/b2.sql .","title":"CSV checks"},{"location":"csv_checks/#csv-checks","text":"This is the default mode when running data_check. data_check expects a SQL file and a CSV file . The SQL file will be executed against the database and the result is compared with the CSV file. If they match, the test is passed, otherwise it fails.","title":"CSV checks"},{"location":"csv_checks/#sql-file","text":"Each SQL file must contain a single SQL query. The query will be executed in the database, hence you must use the SQL dialect that the database in use understands.","title":"SQL file"},{"location":"csv_checks/#templates","text":"SQL files can contain Jinja2 templates. The templates are replaced with the values defined in checks/template.yml . Example: SQL file: select '{{template_value}}' as test checks/template.yml : template_value: ABC CSV file: test ABC","title":"Templates"},{"location":"csv_checks/#csv-format","text":"data_check uses a basic CSV format. Each column is separated by a comma without any space around them. The first line must contain a header. The columns must match the columns in the SQL file. Any columns that do not match between the CSV and the SQL file will be ignored. string_header,int_header,float_header,date_header,null_header string,42,42.1,2020-12-20, # a comment line \"second row\",42,42.1,2020-12-20, Anything after '#' is regarded as a comment. You can use comments to annotate the date as they will be completely ignored. You can escape the start of the comment with '\\' and threat the rest of the line as data. Only the data types strings, decimals and date/timestamps (partially) are supported. Strings can be optionally enclosed in double quotes (\"). Empty strings are treated as null values. data_check recognizes if a column in a SQL query is a date or timestamp and converts the columns in the CSV automatically to timestamps. For some databases (PostgreSQL, MySQL) this only works for timestamp/datetime columns, not date columns. The date format in the CSV file must use ISO 8601 , for example \"2021-12-30\" or \"2021-12-30 13:45:56\". Any other data type must be converted to strings/varchar in the SQL query.","title":"CSV format"},{"location":"csv_checks/#empty-dataset-checks","text":"If you expect the result of the SQL query to be empty, you do not have to write a CSV file with the header. Instead, create a file with the ending .empty . Since the column names do not matter, the check will pass, if the SQL query does not return any values. The .empty file can be empty or contain any data. data_check does not read the content from this file. Example: empty_query.sql empty_query.empty","title":"Empty dataset checks"},{"location":"csv_checks/#generating-expectation-files","text":"If you run data_check --generate in a project folder, data_check will execute the query for each SQL file where the CSV file is missing and write the results into the CSV file. You can add --force to overwrite existing CSV files. You can also generate expectation files for pipelines . If you run --generate on a project with pipelines, beware though that the pipelines will be executed!","title":"Generating expectation files"},{"location":"csv_checks/#lookups","text":"Lookups are a way to reuse the result of a query in multiple other queries. You can use lookups for example to return all table names that you want to exclude from your tests and use the table names to filter them from your queries. To create a lookup, put a SQL file in the lookups folder. To use the lookup in a query, use the lookup name like a subquery or list. The lookup name is the SQL filename prefixed with a colon (':') and without the file ending. If a lookup file is in subfolders, each subfolder is part of the lookup name with a double underscore ('__') as the separator between folders and the SQL filename. Only the first column from a lookup query is used, other columns are ignored. Example: some_check.sql : select a from some_table where b in :b1 or a in :sub_lkp__b2 lookups/b1.sql : select 'b' as b union all select 'c' as b lookups/sub_lkp/b2.sql : select 1 as b union all select 2 as b In this example :b1 is loaded from the file lookups/b1.sql and :sub_lkp__b2 from lookups/sub_lkp/b2.sql .","title":"Lookups"},{"location":"development/","text":"Development poetry must be installed first for development. To set up a development environment initially with poetry: poetry install Later, just activate the virtual environment: poetry shell Please use Black to format the code before committing any change: black data_check Testing data_check has two layers of tests: Unit tests pytest is used for unit testing. There are two types of tests for data_check in the test folder: Basic tests for the code and tests against a database. For unit tests an in-memory SQLite database that is integrated into Python is used. Run pytest inside the virtual environment to execute the unit tests. Integration tests Integration tests are using specific databases and run unit tests and data_check test against this database. There are currently four databases used for integration tests: PostgreSQL MySQL Oracle Microsoft SQL Server The integration tests are run via Drone CI . The file .drone.yml is generated from .drone.jsonnet and checked in into the Git repository. To update .drone.yml run drone jsonnet --format --stream . To speed up integration tests the CI pipeline uses local, pre-build docker images. These images are maintained in a separate repository .","title":"Development"},{"location":"development/#development","text":"poetry must be installed first for development. To set up a development environment initially with poetry: poetry install Later, just activate the virtual environment: poetry shell Please use Black to format the code before committing any change: black data_check","title":"Development"},{"location":"development/#testing","text":"data_check has two layers of tests:","title":"Testing"},{"location":"development/#unit-tests","text":"pytest is used for unit testing. There are two types of tests for data_check in the test folder: Basic tests for the code and tests against a database. For unit tests an in-memory SQLite database that is integrated into Python is used. Run pytest inside the virtual environment to execute the unit tests.","title":"Unit tests"},{"location":"development/#integration-tests","text":"Integration tests are using specific databases and run unit tests and data_check test against this database. There are currently four databases used for integration tests: PostgreSQL MySQL Oracle Microsoft SQL Server The integration tests are run via Drone CI . The file .drone.yml is generated from .drone.jsonnet and checked in into the Git repository. To update .drone.yml run drone jsonnet --format --stream . To speed up integration tests the CI pipeline uses local, pre-build docker images. These images are maintained in a separate repository .","title":"Integration tests"},{"location":"example/","text":"Examples data_check sample project This Git repository is also a sample data_check project. Clone the repository, switch to the folder and run data_check: git clone git@github.com:andrjas/data_check.git cd data_check data_check This will run the tests in the checks folder using the default connection as defined in data_check.yml. The result will tell you which tests passed and which failed: checks/generated/generate_before_running.sql: NO EXPECTED RESULTS FILE checks/failing/invalid.sql: FAILED (with exception in checks/failing/invalid.sql) checks/failing/expected_to_fail.sql: FAILED checks/basic/simple_string.sql: PASSED checks/basic/data_types.sql: PASSED checks/basic/float.sql: PASSED checks/basic/unicode_string.sql: PASSED Tests structure You can structure your test in many ways. You can also mix these structures. By pipeline You can structure your tests to run before/after some data pipeline has run: checks/ pipeline1/ pre/ test1.sql test1.csv ... post/ ... pipeline2/ pre/ ... post/ ... By test execution time In a CI environment you can structure your tests after the expected execution time of the tests. checks/ quick_tests/ ... medium_tests/ ... slow_running_tests/ ... This way you can run quick test, for example schema validation, many times during development. Other tests that must process a lot of data can be run less frequently, for example in an integration environment. Database URLs These are the connection strings used in the integration test . Other connection strings can be found in the SQLAlchemy documentation . PostgreSQL postgresql://username:password@db_host:5432/db_name Oracle oracle+cx_oracle://username:password@db_host:1521/?service_name=XEPDB1 MySQL/MariaDB mysql+pymysql://username:password@db_host:3306/db_name Microsoft SQL Server mssql+pyodbc://username:password@db_host:1433/db_name?driver=ODBC+Driver+17+for+SQL+Server","title":"Examples"},{"location":"example/#examples","text":"","title":"Examples"},{"location":"example/#data_check-sample-project","text":"This Git repository is also a sample data_check project. Clone the repository, switch to the folder and run data_check: git clone git@github.com:andrjas/data_check.git cd data_check data_check This will run the tests in the checks folder using the default connection as defined in data_check.yml. The result will tell you which tests passed and which failed: checks/generated/generate_before_running.sql: NO EXPECTED RESULTS FILE checks/failing/invalid.sql: FAILED (with exception in checks/failing/invalid.sql) checks/failing/expected_to_fail.sql: FAILED checks/basic/simple_string.sql: PASSED checks/basic/data_types.sql: PASSED checks/basic/float.sql: PASSED checks/basic/unicode_string.sql: PASSED","title":"data_check sample project"},{"location":"example/#tests-structure","text":"You can structure your test in many ways. You can also mix these structures.","title":"Tests structure"},{"location":"example/#by-pipeline","text":"You can structure your tests to run before/after some data pipeline has run: checks/ pipeline1/ pre/ test1.sql test1.csv ... post/ ... pipeline2/ pre/ ... post/ ...","title":"By pipeline"},{"location":"example/#by-test-execution-time","text":"In a CI environment you can structure your tests after the expected execution time of the tests. checks/ quick_tests/ ... medium_tests/ ... slow_running_tests/ ... This way you can run quick test, for example schema validation, many times during development. Other tests that must process a lot of data can be run less frequently, for example in an integration environment.","title":"By test execution time"},{"location":"example/#database-urls","text":"These are the connection strings used in the integration test . Other connection strings can be found in the SQLAlchemy documentation .","title":"Database URLs"},{"location":"example/#postgresql","text":"postgresql://username:password@db_host:5432/db_name","title":"PostgreSQL"},{"location":"example/#oracle","text":"oracle+cx_oracle://username:password@db_host:1521/?service_name=XEPDB1","title":"Oracle"},{"location":"example/#mysqlmariadb","text":"mysql+pymysql://username:password@db_host:3306/db_name","title":"MySQL/MariaDB"},{"location":"example/#microsoft-sql-server","text":"mssql+pyodbc://username:password@db_host:1433/db_name?driver=ODBC+Driver+17+for+SQL+Server","title":"Microsoft SQL Server"},{"location":"howtos/","text":"HowTos This is a collection of various things you can do with data_check. Use another configuration file data_check --config other_config_file.yml Run checks in multiple folders Run all test in the folders check_folder_1 and check_folder_2 : data_check check_folder_1 check_folder_2","title":"HowTos"},{"location":"howtos/#howtos","text":"This is a collection of various things you can do with data_check.","title":"HowTos"},{"location":"howtos/#use-another-configuration-file","text":"data_check --config other_config_file.yml","title":"Use another configuration file"},{"location":"howtos/#run-checks-in-multiple-folders","text":"Run all test in the folders check_folder_1 and check_folder_2 : data_check check_folder_1 check_folder_2","title":"Run checks in multiple folders"},{"location":"install/","text":"Installing data_check Depending on your system there are many ways to install data_check. Generally, the steps are always the same: create a virtual environment activate the virtual environment optionally: install pip install data_check You should then be able to run data_check . To run data_check next time in a new terminal you must just activate the virtual environment. Note: While the tool is called 'data_check' the package that you install is called 'data-check'. With pipx The easiest way to install data_check is to use pipx : pipx install data-check To upgrade data_check via pipx: pipx upgrade data-check With Anaconda/Miniconda Create a new conda environment: conda create --name data_check python=3.8 Activate the environment: conda activate data_check and install pip: conda install pip Install data_check via pip: python3 -m pip install data-check To upgrade data_check in the conda environment: python3 -m pip install data-check --upgrade With venv Create a virtual environment ( <venv> point to a relative or absolute path, e.g. c:\\venvs\\data_check) python3 -m venv <venv> Activate the environment: Bash: source <venv>/bin/activate PowerShell: <venv>\\Scripts\\Activate.ps1 The Python documentation has more options how to enable a virtual environment. Install data_check via pip: python3 -m pip install data-check To upgrade data_check in the virtual environment: python3 -m pip install data-check --upgrade Databases Installing data_check alone will only support SQLite, which is bundled with Python. You need additional drivers for other databases. See https://docs.sqlalchemy.org/en/14/dialects/index.html for all possible drivers and how to install them. With pipx you can install the drivers with pipx inject data-check <drivername> (watch the minus sign in data-check instead of the underscore). In a virtual environment you just activate the environment and run pip install <drivername> . Some drivers need additional dependencies. Here are the drivers used for testing data_check: PostgreSQL psycopg2-binary should work on most systems without any additional dependencies. You can use data-check[postgres] to install data_check directly with psycopg2-binary : e.g. with pipx: pipx install data-check[postgres] MySQL/MariaDB PyMySQL as described in https://pypi.org/project/PyMySQL/ with additional cryptography dependencies. Use pipx install data-check[mysql] to install data_check with PyMySQL[rsa] . Microsoft SQL Server pyodbc needs unixodbc and the development package (unixodbc-dev) on Linux. Additionally you must install the Microsoft ODBC driver for SQL Server on your system. Use pipx install data-check[mssql] to install data_check with pyodbc . Oracle cx_Oracle needs Oracle client libraries to work. https://cx-oracle.readthedocs.io/en/latest/user_guide/installation.html shows how to install them. Use pipx install data-check[oracle] to install data_check with cx_Oracle .","title":"Installation"},{"location":"install/#installing-data_check","text":"Depending on your system there are many ways to install data_check. Generally, the steps are always the same: create a virtual environment activate the virtual environment optionally: install pip install data_check You should then be able to run data_check . To run data_check next time in a new terminal you must just activate the virtual environment. Note: While the tool is called 'data_check' the package that you install is called 'data-check'.","title":"Installing data_check"},{"location":"install/#with-pipx","text":"The easiest way to install data_check is to use pipx : pipx install data-check To upgrade data_check via pipx: pipx upgrade data-check","title":"With pipx"},{"location":"install/#with-anacondaminiconda","text":"Create a new conda environment: conda create --name data_check python=3.8 Activate the environment: conda activate data_check and install pip: conda install pip Install data_check via pip: python3 -m pip install data-check To upgrade data_check in the conda environment: python3 -m pip install data-check --upgrade","title":"With Anaconda/Miniconda"},{"location":"install/#with-venv","text":"Create a virtual environment ( <venv> point to a relative or absolute path, e.g. c:\\venvs\\data_check) python3 -m venv <venv> Activate the environment: Bash: source <venv>/bin/activate PowerShell: <venv>\\Scripts\\Activate.ps1 The Python documentation has more options how to enable a virtual environment. Install data_check via pip: python3 -m pip install data-check To upgrade data_check in the virtual environment: python3 -m pip install data-check --upgrade","title":"With venv"},{"location":"install/#databases","text":"Installing data_check alone will only support SQLite, which is bundled with Python. You need additional drivers for other databases. See https://docs.sqlalchemy.org/en/14/dialects/index.html for all possible drivers and how to install them. With pipx you can install the drivers with pipx inject data-check <drivername> (watch the minus sign in data-check instead of the underscore). In a virtual environment you just activate the environment and run pip install <drivername> . Some drivers need additional dependencies. Here are the drivers used for testing data_check:","title":"Databases"},{"location":"install/#postgresql","text":"psycopg2-binary should work on most systems without any additional dependencies. You can use data-check[postgres] to install data_check directly with psycopg2-binary : e.g. with pipx: pipx install data-check[postgres]","title":"PostgreSQL"},{"location":"install/#mysqlmariadb","text":"PyMySQL as described in https://pypi.org/project/PyMySQL/ with additional cryptography dependencies. Use pipx install data-check[mysql] to install data_check with PyMySQL[rsa] .","title":"MySQL/MariaDB"},{"location":"install/#microsoft-sql-server","text":"pyodbc needs unixodbc and the development package (unixodbc-dev) on Linux. Additionally you must install the Microsoft ODBC driver for SQL Server on your system. Use pipx install data-check[mssql] to install data_check with pyodbc .","title":"Microsoft SQL Server"},{"location":"install/#oracle","text":"cx_Oracle needs Oracle client libraries to work. https://cx-oracle.readthedocs.io/en/latest/user_guide/installation.html shows how to install them. Use pipx install data-check[oracle] to install data_check with cx_Oracle .","title":"Oracle"},{"location":"pipelines/","text":"Pipelines If data_check finds a file named data_check_pipeline.yml in a folder, it will treat this folder as a pipeline check. Instead of running CSV checks it will execute the steps in the YAML file. Example Example project with a pipeline: data_check.yml checks/ some_test.sql # this test will run in parallel to the pipeline test some_test.csv sample_pipeline/ data_check_pipeline.yml # configuration for the pipeline data/ my_schema.some_table.csv # data for a table data2/ some_data.csv # other data some_checks/ # folder with CSV checks check1.sql check1.csl ... run_this.sql # a SQL file that will be executed cleanup.sql other_pipeline/ # you can have multiple pipelines that will run in parallel data_check_pipeline.yml ... The file sample_pipeline/data_check_pipeline.yml can look like this: steps: # this will truncate the table my_schema.some_table and load it with the data from data/my_schema.some_table.csv - load_tables: data # this will execute the SQL statement in run_this.sql - sql_file: run_this.sql # this will append the data from data2/some_data.csv to my_schema.other_table - load: file: data2/some_data.csv table: my_schema.other_table load_mode: append # this will run a python script and pass the connection name - cmd: \"python3 /path/to/my_pipeline.py --connection {{CONNECTION}}\" # this will run the CSV checks in the some_checks folder - check: some_checks - always_run: - sql_file: run_this_always.sql Pipeline checks and simple CSV checks can coexist in a project. Pipeline configuration data_check_pipeline.yml is a YAML file with steps as its main element that contains a list of steps. steps steps is a list of steps that are executed in the pipeline sequentially. If any of the steps fail the pipeline will fail and the following steps will not be executed. Most steps have a list of files as a parameter. If you only need a single file/path, you can usually use a short form: step_name: path . check check will run CSV checks in a given folder or from a single file. It is like running data_check with this parameter. All checks will be performed in parallel. Short form: - check: some_checks Long form: - check: files: - some_checks - some/other/checks.sql You can also omit files : - check: - some_checks - some/other/checks.sql load load is like calling data_check --load ... . This will load a CSV file into a table. - load: file: check/date_test.csv table: temp.date_test load_mode: append You can omit load_mode . Then the default mode truncate will be used. load_tables load_tables is like calling data_check --load-tables ... . This will load one or more tables from CSV files and infer the table name from the file name. Like with --load-tables the path before the filename has no impact on the inferred table name, only the file name itself. Short form: - load_tables: some_path/schema.table_name.csv Long form: - load_tables: files: - some_path - some/other/path/schema.other_table.csv load_mode: append You can omit load_mode . Then the default mode truncate will be used. You can also omit files : - load_tables: - some_path - some/other/path/schema.other_table.csv sql_files sql_files is like calling data_check --sql-files ... . This will run a SQL file or all SQL files in a folder against the configured database. All SQL files are executed in parallel. If you need to execute a file after another file, you need to call sql_files twice. sql_file is an alias for sql_files . Short form: - sql_files: some_file.sql Using the alias: - sql_file: some_file.sql Long form: - sql_files: files: - some_file.sql - some_path You can also omit files : - sql_files: - some_file.sql - some_path sql sql is like calling data_check --sql ... . This will execute a SQL statement given as the parameter. If the SQL is a query, the result will be printed as CSV. Short form: - sql: select 1 as a, 'b' as t Long form: - sql: query: select 1 as a, 'b' as t With output to write a CSV file: - sql: query: select 1 as a, 'b' as t output: result.csv output is relative to the pipeline path, unless an absolute path is specified, for example '{{PROJECT_PATH}}/result.csv'. cmd cmd will call any script or program. The commands will be executed sequentially. The optional print parameter can disable console output of the command. Short form: - cmd: echo \"test\" Long form: - cmd: commands: - echo \"test\" - script/to/start/pipeline.sh print: false With print: false no output is printed from the commands. You can also omit commands : - cmd: - echo \"test\" - script/to/start/pipeline.sh always_run always_run is a container for other steps. These steps will always be executed, even if any other step fail. If always_run is between other steps, it will be executed in order. Example: steps: - sql_file: might_fail.sql - always_run: - sql_file: run_after_failing.sql - cmd: some_script.sh - cmd: other_script.sh - always_run: - sql_file: finish.sql In this example, if might_fail.sql fails, run_after_failing.sql , some_script.sh and finish.sql will be run in this order. If might_fail.sql does not fail, other_script.sh is executed after run_after_failing.sql and some_script.sh . finish.sql will then run at the end (even when other_script.sh fails). nested pipelines Pipelines can be nested inside other pipelines. Passing a folder with a data_check_pipeline.yml to check will run the pipeline: - check: - some_checks - folder_with_a_pipeline Parameters in pipelines You can use some predefined parameters in a pipeline definition: CONNECTION: The name of the connection used for this run. CONNECTION_STRING: The connection string as defined in data_check.yml used for the connection. PROJECT_PATH: The path of the data_check project (the folder containing data_check.yml ). PIPELINE_PATH: The absolute path to the pipeline (the folder containing data_check_pipeline.yml ). PIPELINE_NAME: The name of the folder containing data_check_pipeline.yml . Generating pipeline checks Like generating expectation files you can also run data_check --generate for a pipeline. In this mode the pipeline is executed, but each check step will generate the CSV files instead of running the actual checks. Adding --force will overwrite existing CSV files.","title":"Pipelines"},{"location":"pipelines/#pipelines","text":"If data_check finds a file named data_check_pipeline.yml in a folder, it will treat this folder as a pipeline check. Instead of running CSV checks it will execute the steps in the YAML file.","title":"Pipelines"},{"location":"pipelines/#example","text":"Example project with a pipeline: data_check.yml checks/ some_test.sql # this test will run in parallel to the pipeline test some_test.csv sample_pipeline/ data_check_pipeline.yml # configuration for the pipeline data/ my_schema.some_table.csv # data for a table data2/ some_data.csv # other data some_checks/ # folder with CSV checks check1.sql check1.csl ... run_this.sql # a SQL file that will be executed cleanup.sql other_pipeline/ # you can have multiple pipelines that will run in parallel data_check_pipeline.yml ... The file sample_pipeline/data_check_pipeline.yml can look like this: steps: # this will truncate the table my_schema.some_table and load it with the data from data/my_schema.some_table.csv - load_tables: data # this will execute the SQL statement in run_this.sql - sql_file: run_this.sql # this will append the data from data2/some_data.csv to my_schema.other_table - load: file: data2/some_data.csv table: my_schema.other_table load_mode: append # this will run a python script and pass the connection name - cmd: \"python3 /path/to/my_pipeline.py --connection {{CONNECTION}}\" # this will run the CSV checks in the some_checks folder - check: some_checks - always_run: - sql_file: run_this_always.sql Pipeline checks and simple CSV checks can coexist in a project.","title":"Example"},{"location":"pipelines/#pipeline-configuration","text":"data_check_pipeline.yml is a YAML file with steps as its main element that contains a list of steps.","title":"Pipeline configuration"},{"location":"pipelines/#steps","text":"steps is a list of steps that are executed in the pipeline sequentially. If any of the steps fail the pipeline will fail and the following steps will not be executed. Most steps have a list of files as a parameter. If you only need a single file/path, you can usually use a short form: step_name: path .","title":"steps"},{"location":"pipelines/#check","text":"check will run CSV checks in a given folder or from a single file. It is like running data_check with this parameter. All checks will be performed in parallel. Short form: - check: some_checks Long form: - check: files: - some_checks - some/other/checks.sql You can also omit files : - check: - some_checks - some/other/checks.sql","title":"check"},{"location":"pipelines/#load","text":"load is like calling data_check --load ... . This will load a CSV file into a table. - load: file: check/date_test.csv table: temp.date_test load_mode: append You can omit load_mode . Then the default mode truncate will be used.","title":"load"},{"location":"pipelines/#load_tables","text":"load_tables is like calling data_check --load-tables ... . This will load one or more tables from CSV files and infer the table name from the file name. Like with --load-tables the path before the filename has no impact on the inferred table name, only the file name itself. Short form: - load_tables: some_path/schema.table_name.csv Long form: - load_tables: files: - some_path - some/other/path/schema.other_table.csv load_mode: append You can omit load_mode . Then the default mode truncate will be used. You can also omit files : - load_tables: - some_path - some/other/path/schema.other_table.csv","title":"load_tables"},{"location":"pipelines/#sql_files","text":"sql_files is like calling data_check --sql-files ... . This will run a SQL file or all SQL files in a folder against the configured database. All SQL files are executed in parallel. If you need to execute a file after another file, you need to call sql_files twice. sql_file is an alias for sql_files . Short form: - sql_files: some_file.sql Using the alias: - sql_file: some_file.sql Long form: - sql_files: files: - some_file.sql - some_path You can also omit files : - sql_files: - some_file.sql - some_path","title":"sql_files"},{"location":"pipelines/#sql","text":"sql is like calling data_check --sql ... . This will execute a SQL statement given as the parameter. If the SQL is a query, the result will be printed as CSV. Short form: - sql: select 1 as a, 'b' as t Long form: - sql: query: select 1 as a, 'b' as t With output to write a CSV file: - sql: query: select 1 as a, 'b' as t output: result.csv output is relative to the pipeline path, unless an absolute path is specified, for example '{{PROJECT_PATH}}/result.csv'.","title":"sql"},{"location":"pipelines/#cmd","text":"cmd will call any script or program. The commands will be executed sequentially. The optional print parameter can disable console output of the command. Short form: - cmd: echo \"test\" Long form: - cmd: commands: - echo \"test\" - script/to/start/pipeline.sh print: false With print: false no output is printed from the commands. You can also omit commands : - cmd: - echo \"test\" - script/to/start/pipeline.sh","title":"cmd"},{"location":"pipelines/#always_run","text":"always_run is a container for other steps. These steps will always be executed, even if any other step fail. If always_run is between other steps, it will be executed in order. Example: steps: - sql_file: might_fail.sql - always_run: - sql_file: run_after_failing.sql - cmd: some_script.sh - cmd: other_script.sh - always_run: - sql_file: finish.sql In this example, if might_fail.sql fails, run_after_failing.sql , some_script.sh and finish.sql will be run in this order. If might_fail.sql does not fail, other_script.sh is executed after run_after_failing.sql and some_script.sh . finish.sql will then run at the end (even when other_script.sh fails).","title":"always_run"},{"location":"pipelines/#nested-pipelines","text":"Pipelines can be nested inside other pipelines. Passing a folder with a data_check_pipeline.yml to check will run the pipeline: - check: - some_checks - folder_with_a_pipeline","title":"nested pipelines"},{"location":"pipelines/#parameters-in-pipelines","text":"You can use some predefined parameters in a pipeline definition: CONNECTION: The name of the connection used for this run. CONNECTION_STRING: The connection string as defined in data_check.yml used for the connection. PROJECT_PATH: The path of the data_check project (the folder containing data_check.yml ). PIPELINE_PATH: The absolute path to the pipeline (the folder containing data_check_pipeline.yml ). PIPELINE_NAME: The name of the folder containing data_check_pipeline.yml .","title":"Parameters in pipelines"},{"location":"pipelines/#generating-pipeline-checks","text":"Like generating expectation files you can also run data_check --generate for a pipeline. In this mode the pipeline is executed, but each check step will generate the CSV files instead of running the actual checks. Adding --force will overwrite existing CSV files.","title":"Generating pipeline checks"},{"location":"usage/","text":"Usage Commands data_check - Run data_check against the default connection in the checks folder. data_check some_folder - Run data_check against the default connection in the some_folder folder. data_check some_folder/some_file.sql - Run data_check against the default connection for a single test. data_check -c/--connection CONNECTION - use another connection than the default. data_check -n/--workers WORKERS - use WORKERS threads to run the queries (default: 4). data_check --print - print failed results data. data_check --print-format FORMAT - format for printing failed results (csv (default), pandas, json). data_check --print-json - shortcut for \"--print --print-format json\". data_check -g/--generate - generate expectation files if they don't exist. data_check --force - when set, --generate will overwrite files. data_check --config CONFIG - config file to use (default: data_check.yml). data_check --load PATH --table - load table data from a csv into the table data_check --load-mode MODE - how to load the table: truncate (default), append or replace. Use with --load or --load-tables. data_check --load-tables some_folder/or/some_file.csv - load tables from a list of csv files. data_check --sql-files some_folder/or/some_file.sql - run any SQL script in a list of SQL files. data_check --sql-file some_folder/or/some_file.sql - alias for --sql-files. data_check --sql \"SQL statement\" - run any SQL statement. Print result as CSV if it is a query. data_check -o/--output PATH - output path for --sql. data_check --ping - tries to connect to the database. data_check --quiet - do not print any output. data_check --verbose - print verbose output. data_check --traceback - print traceback output for debugging. data_check --log LOGFILE - write output to a log file. data_check --version - Show the version and exit. data_check --help - show this help message and exit. Exit codes Possible exit codes: Exit code 0: All tests run successfully. Exit code 1: At least one test failed. Logging --log will write the output to a log file and to the console. With --quiet the output will only be printed to the log file. If the file exists, the output will be appended. You can also set the log file in data_check.yml: log: logfile.txt Loading data into tables Sometimes you need to populate tables with some data before running pipeline tests. With data_check you can use CSV or Excel files to load data into tables. The CSV format is the same as used for testing. The header in the CSV file or the first row in the Excel file must match the columns in the table. Additionally, Excel cells can use its native date/datetime format. If the table doesn't exist, it will be created. The schema and table names are always case-insensitive, as long as the database supports it. Otherwise, they are lowercased. Loading data into a single table To load data from some CSV or Excel file into a table, you can use data_check --load path/to/some_file.[csv/xlsx] --table schema.table_name . This will truncate the table and load the content of the file into the table. You can specify different load modes if you do not want to truncate the table. Loading data into multiple tables You can use multiple CSV and Excel files or a whole folder to load the data into tables. The table name will be derived from the file name. data_check --load-tables path/schema.table_1.csv - this will load the data from the CSV file into the table schema.table_1 data_check --load-tables path/schema.table_2.xlsx - this will load the data from the Excel file into the table schema.table_2 data_check --load-tables path/to/some_folder - this will load the data from all CSV and Excel files in this folder into tables matching the file names. The path will be searched recursively for CSV and Excel files. The folder structure doesn't matter when matching the table names, only the file name matters. Load modes There are multiple modes that control the data loading: truncate - truncate the table before loading the data. replace - drop the table and recreate it based on the columns in the CSV or Excel file. append - do not touch the table before loading the data. By default, the tables will be truncated before loading the data. You can use the other modes with --load-mode : data_check --load file.csv --table table_name --load-mode replace data_check --load-tables path/to/tables_folder --load-mode append CSV and data types When loading data from CSV files, data_check (or more precisely: pandas ) will infer the data types from the file. When loading the data into the table, the database will usually implicitly convert the data types. This works good for simple data types like strings and numbers. If you need to load date types (or timestamps) and the table has a date column, data_check will try to convert these columns in the CSV file into a datetime. This doesn't work when using --load-mode replace since the table will be dropped before it can be analyzed. This will probably result in a varchar column instead of date. Use ISO 8601 for dates, like for [CSV checks]((csv_checks.md#csv-format) Executing arbitrary SQL code You can run any SQL file against the database by using the --sql-files command: data_check --sql-files sql_file.sql other_file.sql or a whole folder recursively: data_check --sql-files some/folder/with/sql_files All files are run in parallel. If you have dependencies between the files, data_check must be called sequentially for each file. Multiple statements in a SQL file usually must be inside an anonymous block. MySQL doesn't support this however. --sql-file is an alias for --sql-files . Use it to indicate, that you only want to run a single file. You can also run a SQL statement directly from the command line: data_check --sql \"select * from tableX\" This will execute the query and, if it is a query, print the result as CSV. You can also write the result into a file: data_check --sql \"select * from tableX\" --output some_file.csv","title":"Usage"},{"location":"usage/#usage","text":"","title":"Usage"},{"location":"usage/#commands","text":"data_check - Run data_check against the default connection in the checks folder. data_check some_folder - Run data_check against the default connection in the some_folder folder. data_check some_folder/some_file.sql - Run data_check against the default connection for a single test. data_check -c/--connection CONNECTION - use another connection than the default. data_check -n/--workers WORKERS - use WORKERS threads to run the queries (default: 4). data_check --print - print failed results data. data_check --print-format FORMAT - format for printing failed results (csv (default), pandas, json). data_check --print-json - shortcut for \"--print --print-format json\". data_check -g/--generate - generate expectation files if they don't exist. data_check --force - when set, --generate will overwrite files. data_check --config CONFIG - config file to use (default: data_check.yml). data_check --load PATH --table - load table data from a csv into the table data_check --load-mode MODE - how to load the table: truncate (default), append or replace. Use with --load or --load-tables. data_check --load-tables some_folder/or/some_file.csv - load tables from a list of csv files. data_check --sql-files some_folder/or/some_file.sql - run any SQL script in a list of SQL files. data_check --sql-file some_folder/or/some_file.sql - alias for --sql-files. data_check --sql \"SQL statement\" - run any SQL statement. Print result as CSV if it is a query. data_check -o/--output PATH - output path for --sql. data_check --ping - tries to connect to the database. data_check --quiet - do not print any output. data_check --verbose - print verbose output. data_check --traceback - print traceback output for debugging. data_check --log LOGFILE - write output to a log file. data_check --version - Show the version and exit. data_check --help - show this help message and exit.","title":"Commands"},{"location":"usage/#exit-codes","text":"Possible exit codes: Exit code 0: All tests run successfully. Exit code 1: At least one test failed.","title":"Exit codes"},{"location":"usage/#logging","text":"--log will write the output to a log file and to the console. With --quiet the output will only be printed to the log file. If the file exists, the output will be appended. You can also set the log file in data_check.yml: log: logfile.txt","title":"Logging"},{"location":"usage/#loading-data-into-tables","text":"Sometimes you need to populate tables with some data before running pipeline tests. With data_check you can use CSV or Excel files to load data into tables. The CSV format is the same as used for testing. The header in the CSV file or the first row in the Excel file must match the columns in the table. Additionally, Excel cells can use its native date/datetime format. If the table doesn't exist, it will be created. The schema and table names are always case-insensitive, as long as the database supports it. Otherwise, they are lowercased.","title":"Loading data into tables"},{"location":"usage/#loading-data-into-a-single-table","text":"To load data from some CSV or Excel file into a table, you can use data_check --load path/to/some_file.[csv/xlsx] --table schema.table_name . This will truncate the table and load the content of the file into the table. You can specify different load modes if you do not want to truncate the table.","title":"Loading data into a single table"},{"location":"usage/#loading-data-into-multiple-tables","text":"You can use multiple CSV and Excel files or a whole folder to load the data into tables. The table name will be derived from the file name. data_check --load-tables path/schema.table_1.csv - this will load the data from the CSV file into the table schema.table_1 data_check --load-tables path/schema.table_2.xlsx - this will load the data from the Excel file into the table schema.table_2 data_check --load-tables path/to/some_folder - this will load the data from all CSV and Excel files in this folder into tables matching the file names. The path will be searched recursively for CSV and Excel files. The folder structure doesn't matter when matching the table names, only the file name matters.","title":"Loading data into multiple tables"},{"location":"usage/#load-modes","text":"There are multiple modes that control the data loading: truncate - truncate the table before loading the data. replace - drop the table and recreate it based on the columns in the CSV or Excel file. append - do not touch the table before loading the data. By default, the tables will be truncated before loading the data. You can use the other modes with --load-mode : data_check --load file.csv --table table_name --load-mode replace data_check --load-tables path/to/tables_folder --load-mode append","title":"Load modes"},{"location":"usage/#csv-and-data-types","text":"When loading data from CSV files, data_check (or more precisely: pandas ) will infer the data types from the file. When loading the data into the table, the database will usually implicitly convert the data types. This works good for simple data types like strings and numbers. If you need to load date types (or timestamps) and the table has a date column, data_check will try to convert these columns in the CSV file into a datetime. This doesn't work when using --load-mode replace since the table will be dropped before it can be analyzed. This will probably result in a varchar column instead of date. Use ISO 8601 for dates, like for [CSV checks]((csv_checks.md#csv-format)","title":"CSV and data types"},{"location":"usage/#executing-arbitrary-sql-code","text":"You can run any SQL file against the database by using the --sql-files command: data_check --sql-files sql_file.sql other_file.sql or a whole folder recursively: data_check --sql-files some/folder/with/sql_files All files are run in parallel. If you have dependencies between the files, data_check must be called sequentially for each file. Multiple statements in a SQL file usually must be inside an anonymous block. MySQL doesn't support this however. --sql-file is an alias for --sql-files . Use it to indicate, that you only want to run a single file. You can also run a SQL statement directly from the command line: data_check --sql \"select * from tableX\" This will execute the query and, if it is a query, print the result as CSV. You can also write the result into a file: data_check --sql \"select * from tableX\" --output some_file.csv","title":"Executing arbitrary SQL code"}]}