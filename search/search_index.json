{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to data_check data_check is a simple data validation tool. In its most basic form, it will execute SQL queries and compare the results against CSV or Excel files. But there are more advanced features: Features CSV checks : compare SQL queries against CSV files Excel support: Use Excel (xlsx) instead of CSV multiple environments (databases) in the configuration file populate tables from CSV or Excel files execute any SQL files on a database more complex pipelines run any script/command (via pipelines) simplified checks for empty datasets and full table comparison lookups to reuse the same data in multiple queries test data generation Database support data_check should work with any database that works with SQLAlchemy . Currently data_check is tested against PostgreSQL, MySQL, SQLite, Oracle and Microsoft SQL Server. Why? data_check tries to solve a simple problem in data domains: during development you create multiple SQL queries to validate your data and compare the result manually. With data_check you can organize your data tests and run them automatically, for example in a CI pipeline. How to get started First install data_check. You can then try data_check with the sample project . To create a new project folder write a data_check.yml for the configuration and put your test files in the checks folder. Project layout data_check has a simple layout for projects: a single configuration file and a folder with the test files. You can also organize the test files in subfolders. data_check.yml # The configuration file checks/ # Default folder for data tests some_test.sql # SQL file with the query to run against the database some_test.csv # CSV file with the expected result other_test.sql # SQL file with another test other_test.xlsx # Expected result for other_test.sql in an Excel file empty_result.sql # SQL file with a result set that is expected to be empty empty_result.empty # empty file for empty_result.sql subfolder/ # Tests can be nested in subfolders Configuration data_check uses the data_check.yml file in the current folder for configuration. This is a simple YAML file: default_connection: con connections: con: connection-string Under connections you can put multiple connection strings with names. default_connection is the connection name that data_check uses when no additional arguments are given. You can also use environment variables in the connection string to store the credentials outside data_check.yml (e.g. postgresql://postgres:${POSTGRES_PWD}@db:5432 ). Test files The test files are organized in the checks folder and its subfolders. data_check will run the queries for each SQL file in these folders and compare the results with the corresponding CSV files. The CSV file must be named exactly like the SQL file, only with different file endings. Instead of writing the CSV files manually, you can also generate them from the SQL files. How it works data_check uses pandas and SQLAlchemy internally. The SQL files are executed and converted to DataFrames. The CSV files are also parsed to DataFrames. Both DataFrames are then merged with a full outer join. If some rows differ, the test is considered as failed.","title":"Home"},{"location":"#welcome-to-data_check","text":"data_check is a simple data validation tool. In its most basic form, it will execute SQL queries and compare the results against CSV or Excel files. But there are more advanced features:","title":"Welcome to data_check"},{"location":"#features","text":"CSV checks : compare SQL queries against CSV files Excel support: Use Excel (xlsx) instead of CSV multiple environments (databases) in the configuration file populate tables from CSV or Excel files execute any SQL files on a database more complex pipelines run any script/command (via pipelines) simplified checks for empty datasets and full table comparison lookups to reuse the same data in multiple queries test data generation","title":"Features"},{"location":"#database-support","text":"data_check should work with any database that works with SQLAlchemy . Currently data_check is tested against PostgreSQL, MySQL, SQLite, Oracle and Microsoft SQL Server.","title":"Database support"},{"location":"#why","text":"data_check tries to solve a simple problem in data domains: during development you create multiple SQL queries to validate your data and compare the result manually. With data_check you can organize your data tests and run them automatically, for example in a CI pipeline.","title":"Why?"},{"location":"#how-to-get-started","text":"First install data_check. You can then try data_check with the sample project . To create a new project folder write a data_check.yml for the configuration and put your test files in the checks folder.","title":"How to get started"},{"location":"#project-layout","text":"data_check has a simple layout for projects: a single configuration file and a folder with the test files. You can also organize the test files in subfolders. data_check.yml # The configuration file checks/ # Default folder for data tests some_test.sql # SQL file with the query to run against the database some_test.csv # CSV file with the expected result other_test.sql # SQL file with another test other_test.xlsx # Expected result for other_test.sql in an Excel file empty_result.sql # SQL file with a result set that is expected to be empty empty_result.empty # empty file for empty_result.sql subfolder/ # Tests can be nested in subfolders","title":"Project layout"},{"location":"#configuration","text":"data_check uses the data_check.yml file in the current folder for configuration. This is a simple YAML file: default_connection: con connections: con: connection-string Under connections you can put multiple connection strings with names. default_connection is the connection name that data_check uses when no additional arguments are given. You can also use environment variables in the connection string to store the credentials outside data_check.yml (e.g. postgresql://postgres:${POSTGRES_PWD}@db:5432 ).","title":"Configuration"},{"location":"#test-files","text":"The test files are organized in the checks folder and its subfolders. data_check will run the queries for each SQL file in these folders and compare the results with the corresponding CSV files. The CSV file must be named exactly like the SQL file, only with different file endings. Instead of writing the CSV files manually, you can also generate them from the SQL files.","title":"Test files"},{"location":"#how-it-works","text":"data_check uses pandas and SQLAlchemy internally. The SQL files are executed and converted to DataFrames. The CSV files are also parsed to DataFrames. Both DataFrames are then merged with a full outer join. If some rows differ, the test is considered as failed.","title":"How it works"},{"location":"csv_checks/","text":"CSV checks This is the default mode when running data_check. data_check expects a SQL file and a CSV file . The SQL file will be executed against the database and the result is compared with the CSV file. If they match, the test is passed, otherwise it fails. SQL file Each SQL file must contain a single SQL query. The query will be executed in the database, hence you must use the SQL dialect that the database in use understands. Templates SQL files can contain Jinja2 templates. The templates are replaced with the values defined in checks/template.yml . Example: SQL file: select '{{template_value}}' as test checks/template.yml : template_value: ABC CSV file: test ABC CSV format data_check uses a basic CSV format. Each column is separated by a comma without any space around them. The first line must contain a header. The columns must match the columns in the SQL file. Any columns that do not match between the CSV and the SQL file will be ignored. string_header,int_header,float_header,date_header,null_header string,42,42.1,2020-12-20, # a comment line \"second row\",42,42.1,2020-12-20, Anything after '#' is regarded as a comment. You can use comments to annotate the date as they will be completely ignored. You can escape the start of the comment with '\\' and threat the rest of the line as data. Only the data types strings, decimals and date/timestamps (partially) are supported. Strings can be optionally enclosed in double quotes (\"). Empty strings are treated as null values. data_check recognizes if a column in a SQL query is a date or timestamp and converts the columns in the CSV automatically to timestamps. For some databases (PostgreSQL, MySQL) this only works for timestamp/datetime columns, not date columns. The date format in the CSV file must use ISO 8601 , for example \"2021-12-30\" or \"2021-12-30 13:45:56\". Any other data type must be converted to strings/varchar in the SQL query. Empty dataset checks If you expect the result of the SQL query to be empty, you do not have to write a CSV file with the header. Instead, create a file with the ending .empty . Since the column names do not matter, the check will pass, if the SQL query does not return any values. The .empty file can be empty or contain any data. data_check does not read the content from this file. Example: empty_query.sql empty_query.empty Full table checks To compare the content of a whole table, you can also put a CSV or Excel file without the SQL file. The file must be named after the table name. data_check will only compare the columns in the file. If the table does not have a column from the CSV/Excel header, the test will fail. Example: schema.table_name.csv : column1,column2 data1,data2 ... This will run this SQL query: select column1, column2 from schema.table_name and compare the result against the CSV file. Generating expectation files If you run data_check gen in a project folder, data_check will execute the query for each SQL file where the CSV file is missing and write the results into the CSV file. You can add --force to overwrite existing CSV files. You can also generate expectation files for pipelines . If you run data_check gen on a project with pipelines, beware though that the pipelines will be executed! Lookups Lookups are a way to reuse the result of a query in multiple other queries. You can use lookups for example to return all table names that you want to exclude from your tests and use the table names to filter them from your queries. To create a lookup, put a SQL file in the lookups folder. To use the lookup in a query, use the lookup name like a subquery or list. The lookup name is the SQL filename prefixed with a colon (':') and without the file ending. If a lookup file is in subfolders, each subfolder is part of the lookup name with a double underscore ('__') as the separator between folders and the SQL filename. Only the first column from a lookup query is used, other columns are ignored. Example: some_check.sql : select a from some_table where b in :b1 or a in :sub_lkp__b2 lookups/b1.sql : select 'b' as b union all select 'c' as b lookups/sub_lkp/b2.sql : select 1 as b union all select 2 as b In this example :b1 is loaded from the file lookups/b1.sql and :sub_lkp__b2 from lookups/sub_lkp/b2.sql .","title":"CSV checks"},{"location":"csv_checks/#csv-checks","text":"This is the default mode when running data_check. data_check expects a SQL file and a CSV file . The SQL file will be executed against the database and the result is compared with the CSV file. If they match, the test is passed, otherwise it fails.","title":"CSV checks"},{"location":"csv_checks/#sql-file","text":"Each SQL file must contain a single SQL query. The query will be executed in the database, hence you must use the SQL dialect that the database in use understands.","title":"SQL file"},{"location":"csv_checks/#templates","text":"SQL files can contain Jinja2 templates. The templates are replaced with the values defined in checks/template.yml . Example: SQL file: select '{{template_value}}' as test checks/template.yml : template_value: ABC CSV file: test ABC","title":"Templates"},{"location":"csv_checks/#csv-format","text":"data_check uses a basic CSV format. Each column is separated by a comma without any space around them. The first line must contain a header. The columns must match the columns in the SQL file. Any columns that do not match between the CSV and the SQL file will be ignored. string_header,int_header,float_header,date_header,null_header string,42,42.1,2020-12-20, # a comment line \"second row\",42,42.1,2020-12-20, Anything after '#' is regarded as a comment. You can use comments to annotate the date as they will be completely ignored. You can escape the start of the comment with '\\' and threat the rest of the line as data. Only the data types strings, decimals and date/timestamps (partially) are supported. Strings can be optionally enclosed in double quotes (\"). Empty strings are treated as null values. data_check recognizes if a column in a SQL query is a date or timestamp and converts the columns in the CSV automatically to timestamps. For some databases (PostgreSQL, MySQL) this only works for timestamp/datetime columns, not date columns. The date format in the CSV file must use ISO 8601 , for example \"2021-12-30\" or \"2021-12-30 13:45:56\". Any other data type must be converted to strings/varchar in the SQL query.","title":"CSV format"},{"location":"csv_checks/#empty-dataset-checks","text":"If you expect the result of the SQL query to be empty, you do not have to write a CSV file with the header. Instead, create a file with the ending .empty . Since the column names do not matter, the check will pass, if the SQL query does not return any values. The .empty file can be empty or contain any data. data_check does not read the content from this file. Example: empty_query.sql empty_query.empty","title":"Empty dataset checks"},{"location":"csv_checks/#full-table-checks","text":"To compare the content of a whole table, you can also put a CSV or Excel file without the SQL file. The file must be named after the table name. data_check will only compare the columns in the file. If the table does not have a column from the CSV/Excel header, the test will fail. Example: schema.table_name.csv : column1,column2 data1,data2 ... This will run this SQL query: select column1, column2 from schema.table_name and compare the result against the CSV file.","title":"Full table checks"},{"location":"csv_checks/#generating-expectation-files","text":"If you run data_check gen in a project folder, data_check will execute the query for each SQL file where the CSV file is missing and write the results into the CSV file. You can add --force to overwrite existing CSV files. You can also generate expectation files for pipelines . If you run data_check gen on a project with pipelines, beware though that the pipelines will be executed!","title":"Generating expectation files"},{"location":"csv_checks/#lookups","text":"Lookups are a way to reuse the result of a query in multiple other queries. You can use lookups for example to return all table names that you want to exclude from your tests and use the table names to filter them from your queries. To create a lookup, put a SQL file in the lookups folder. To use the lookup in a query, use the lookup name like a subquery or list. The lookup name is the SQL filename prefixed with a colon (':') and without the file ending. If a lookup file is in subfolders, each subfolder is part of the lookup name with a double underscore ('__') as the separator between folders and the SQL filename. Only the first column from a lookup query is used, other columns are ignored. Example: some_check.sql : select a from some_table where b in :b1 or a in :sub_lkp__b2 lookups/b1.sql : select 'b' as b union all select 'c' as b lookups/sub_lkp/b2.sql : select 1 as b union all select 2 as b In this example :b1 is loaded from the file lookups/b1.sql and :sub_lkp__b2 from lookups/sub_lkp/b2.sql .","title":"Lookups"},{"location":"development/","text":"Development poetry must be installed first for development. To set up a development environment initially with poetry: poetry install Later, just activate the virtual environment: poetry shell Please use Black to format the code before committing any change: black data_check Testing data_check has two layers of tests: Unit tests pytest is used for unit testing. There are two types of tests for data_check in the test folder: Basic tests for the code and tests against a database. For unit tests an in-memory SQLite database that is integrated into Python is used. Run pytest inside the virtual environment to execute the unit tests. Integration tests Integration tests are using specific databases and run unit tests and data_check test against this database. There are currently four databases used for integration tests: PostgreSQL MySQL Oracle Microsoft SQL Server The integration tests are run via Drone CI . The file .drone.yml is generated from .drone.jsonnet and checked in into the Git repository. To update .drone.yml run drone jsonnet --format --stream . To speed up integration tests the CI pipeline uses local, pre-build docker images. These images are maintained in a separate repository .","title":"Development"},{"location":"development/#development","text":"poetry must be installed first for development. To set up a development environment initially with poetry: poetry install Later, just activate the virtual environment: poetry shell Please use Black to format the code before committing any change: black data_check","title":"Development"},{"location":"development/#testing","text":"data_check has two layers of tests:","title":"Testing"},{"location":"development/#unit-tests","text":"pytest is used for unit testing. There are two types of tests for data_check in the test folder: Basic tests for the code and tests against a database. For unit tests an in-memory SQLite database that is integrated into Python is used. Run pytest inside the virtual environment to execute the unit tests.","title":"Unit tests"},{"location":"development/#integration-tests","text":"Integration tests are using specific databases and run unit tests and data_check test against this database. There are currently four databases used for integration tests: PostgreSQL MySQL Oracle Microsoft SQL Server The integration tests are run via Drone CI . The file .drone.yml is generated from .drone.jsonnet and checked in into the Git repository. To update .drone.yml run drone jsonnet --format --stream . To speed up integration tests the CI pipeline uses local, pre-build docker images. These images are maintained in a separate repository .","title":"Integration tests"},{"location":"example/","text":"Examples data_check sample project This Git repository is also a sample data_check project. Clone the repository, switch to the folder and run data_check: git clone git@github.com:andrjas/data_check.git cd data_check/example data_check This will run the tests in the checks folder using the default connection as defined in data_check.yml. The result will tell you which tests passed and which failed: checks/generated/generate_before_running.sql: NO EXPECTED RESULTS FILE checks/failing/invalid.sql: FAILED (with exception in checks/failing/invalid.sql) checks/failing/expected_to_fail.sql: FAILED checks/basic/simple_string.sql: PASSED checks/basic/data_types.sql: PASSED checks/basic/float.sql: PASSED checks/basic/unicode_string.sql: PASSED Tests structure You can structure your test in many ways. You can also mix these structures. By pipeline You can structure your tests to run before/after some data pipeline has run: checks/ pipeline1/ pre/ test1.sql test1.csv ... post/ ... pipeline2/ pre/ ... post/ ... By test execution time In a CI environment you can structure your tests after the expected execution time of the tests. checks/ quick_tests/ ... medium_tests/ ... slow_running_tests/ ... This way you can run quick test, for example schema validation, many times during development. Other tests that must process a lot of data can be run less frequently, for example in an integration environment. Database URLs These are the connection strings used in the integration test . Other connection strings can be found in the SQLAlchemy documentation . PostgreSQL postgresql://username:password@db_host:5432/db_name Oracle oracle+cx_oracle://username:password@db_host:1521/?service_name=XEPDB1 MySQL/MariaDB mysql+pymysql://username:password@db_host:3306/db_name Microsoft SQL Server mssql+pyodbc://username:password@db_host:1433/db_name?driver=ODBC+Driver+17+for+SQL+Server","title":"Examples"},{"location":"example/#examples","text":"","title":"Examples"},{"location":"example/#data_check-sample-project","text":"This Git repository is also a sample data_check project. Clone the repository, switch to the folder and run data_check: git clone git@github.com:andrjas/data_check.git cd data_check/example data_check This will run the tests in the checks folder using the default connection as defined in data_check.yml. The result will tell you which tests passed and which failed: checks/generated/generate_before_running.sql: NO EXPECTED RESULTS FILE checks/failing/invalid.sql: FAILED (with exception in checks/failing/invalid.sql) checks/failing/expected_to_fail.sql: FAILED checks/basic/simple_string.sql: PASSED checks/basic/data_types.sql: PASSED checks/basic/float.sql: PASSED checks/basic/unicode_string.sql: PASSED","title":"data_check sample project"},{"location":"example/#tests-structure","text":"You can structure your test in many ways. You can also mix these structures.","title":"Tests structure"},{"location":"example/#by-pipeline","text":"You can structure your tests to run before/after some data pipeline has run: checks/ pipeline1/ pre/ test1.sql test1.csv ... post/ ... pipeline2/ pre/ ... post/ ...","title":"By pipeline"},{"location":"example/#by-test-execution-time","text":"In a CI environment you can structure your tests after the expected execution time of the tests. checks/ quick_tests/ ... medium_tests/ ... slow_running_tests/ ... This way you can run quick test, for example schema validation, many times during development. Other tests that must process a lot of data can be run less frequently, for example in an integration environment.","title":"By test execution time"},{"location":"example/#database-urls","text":"These are the connection strings used in the integration test . Other connection strings can be found in the SQLAlchemy documentation .","title":"Database URLs"},{"location":"example/#postgresql","text":"postgresql://username:password@db_host:5432/db_name","title":"PostgreSQL"},{"location":"example/#oracle","text":"oracle+cx_oracle://username:password@db_host:1521/?service_name=XEPDB1","title":"Oracle"},{"location":"example/#mysqlmariadb","text":"mysql+pymysql://username:password@db_host:3306/db_name","title":"MySQL/MariaDB"},{"location":"example/#microsoft-sql-server","text":"mssql+pyodbc://username:password@db_host:1433/db_name?driver=ODBC+Driver+17+for+SQL+Server","title":"Microsoft SQL Server"},{"location":"howtos/","text":"HowTos This is a collection of various things you can do with data_check. Use another configuration file data_check --config other_config_file.yml Run checks in multiple folders Run all test in the folders check_folder_1 and check_folder_2 : data_check check_folder_1 check_folder_2","title":"HowTos"},{"location":"howtos/#howtos","text":"This is a collection of various things you can do with data_check.","title":"HowTos"},{"location":"howtos/#use-another-configuration-file","text":"data_check --config other_config_file.yml","title":"Use another configuration file"},{"location":"howtos/#run-checks-in-multiple-folders","text":"Run all test in the folders check_folder_1 and check_folder_2 : data_check check_folder_1 check_folder_2","title":"Run checks in multiple folders"},{"location":"install/","text":"Installing data_check Depending on your system there are many ways to install data_check. Generally, the steps are always the same: create a virtual environment activate the virtual environment optionally: install pip install data_check You should then be able to run data_check . To run data_check next time in a new terminal you must just activate the virtual environment. Note: While the tool is called 'data_check' the package that you install is called 'data-check'. With pipx The easiest way to install data_check is to use pipx : pipx install data-check To upgrade data_check via pipx: pipx upgrade data-check With Anaconda/Miniconda Create a new conda environment: conda create --name data_check python=3.8 Activate the environment: conda activate data_check and install pip: conda install pip Install data_check via pip: python3 -m pip install data-check To upgrade data_check in the conda environment: python3 -m pip install data-check --upgrade With venv Create a virtual environment ( <venv> point to a relative or absolute path, e.g. c:\\venvs\\data_check) python3 -m venv <venv> Activate the environment: Bash: source <venv>/bin/activate PowerShell: <venv>\\Scripts\\Activate.ps1 The Python documentation has more options how to enable a virtual environment. Install data_check via pip: python3 -m pip install data-check To upgrade data_check in the virtual environment: python3 -m pip install data-check --upgrade Databases Installing data_check alone will only support SQLite, which is bundled with Python. You need additional drivers for other databases. See https://docs.sqlalchemy.org/en/14/dialects/index.html for all possible drivers and how to install them. With pipx you can install the drivers with pipx inject data-check <drivername> (watch the minus sign in data-check instead of the underscore). In a virtual environment you just activate the environment and run pip install <drivername> . Some drivers need additional dependencies. Here are the drivers used for testing data_check: PostgreSQL psycopg2-binary should work on most systems without any additional dependencies. You can use data-check[postgres] to install data_check directly with psycopg2-binary : e.g. with pipx: pipx install data-check[postgres] MySQL/MariaDB PyMySQL as described in https://pypi.org/project/PyMySQL/ with additional cryptography dependencies. Use pipx install data-check[mysql] to install data_check with PyMySQL[rsa] . Microsoft SQL Server pyodbc needs unixodbc and the development package (unixodbc-dev) on Linux. Additionally you must install the Microsoft ODBC driver for SQL Server on your system. Use pipx install data-check[mssql] to install data_check with pyodbc . Oracle cx_Oracle needs Oracle client libraries to work. https://cx-oracle.readthedocs.io/en/latest/user_guide/installation.html shows how to install them. Use pipx install data-check[oracle] to install data_check with cx_Oracle .","title":"Installation"},{"location":"install/#installing-data_check","text":"Depending on your system there are many ways to install data_check. Generally, the steps are always the same: create a virtual environment activate the virtual environment optionally: install pip install data_check You should then be able to run data_check . To run data_check next time in a new terminal you must just activate the virtual environment. Note: While the tool is called 'data_check' the package that you install is called 'data-check'.","title":"Installing data_check"},{"location":"install/#with-pipx","text":"The easiest way to install data_check is to use pipx : pipx install data-check To upgrade data_check via pipx: pipx upgrade data-check","title":"With pipx"},{"location":"install/#with-anacondaminiconda","text":"Create a new conda environment: conda create --name data_check python=3.8 Activate the environment: conda activate data_check and install pip: conda install pip Install data_check via pip: python3 -m pip install data-check To upgrade data_check in the conda environment: python3 -m pip install data-check --upgrade","title":"With Anaconda/Miniconda"},{"location":"install/#with-venv","text":"Create a virtual environment ( <venv> point to a relative or absolute path, e.g. c:\\venvs\\data_check) python3 -m venv <venv> Activate the environment: Bash: source <venv>/bin/activate PowerShell: <venv>\\Scripts\\Activate.ps1 The Python documentation has more options how to enable a virtual environment. Install data_check via pip: python3 -m pip install data-check To upgrade data_check in the virtual environment: python3 -m pip install data-check --upgrade","title":"With venv"},{"location":"install/#databases","text":"Installing data_check alone will only support SQLite, which is bundled with Python. You need additional drivers for other databases. See https://docs.sqlalchemy.org/en/14/dialects/index.html for all possible drivers and how to install them. With pipx you can install the drivers with pipx inject data-check <drivername> (watch the minus sign in data-check instead of the underscore). In a virtual environment you just activate the environment and run pip install <drivername> . Some drivers need additional dependencies. Here are the drivers used for testing data_check:","title":"Databases"},{"location":"install/#postgresql","text":"psycopg2-binary should work on most systems without any additional dependencies. You can use data-check[postgres] to install data_check directly with psycopg2-binary : e.g. with pipx: pipx install data-check[postgres]","title":"PostgreSQL"},{"location":"install/#mysqlmariadb","text":"PyMySQL as described in https://pypi.org/project/PyMySQL/ with additional cryptography dependencies. Use pipx install data-check[mysql] to install data_check with PyMySQL[rsa] .","title":"MySQL/MariaDB"},{"location":"install/#microsoft-sql-server","text":"pyodbc needs unixodbc and the development package (unixodbc-dev) on Linux. Additionally you must install the Microsoft ODBC driver for SQL Server on your system. Use pipx install data-check[mssql] to install data_check with pyodbc .","title":"Microsoft SQL Server"},{"location":"install/#oracle","text":"cx_Oracle needs Oracle client libraries to work. https://cx-oracle.readthedocs.io/en/latest/user_guide/installation.html shows how to install them. Use pipx install data-check[oracle] to install data_check with cx_Oracle .","title":"Oracle"},{"location":"pipelines/","text":"Pipelines If data_check finds a file named data_check_pipeline.yml in a folder, it will treat this folder as a pipeline check. Instead of running CSV checks it will execute the steps in the YAML file. Example Example project with a pipeline: data_check.yml checks/ some_test.sql # this test will run in parallel to the pipeline test some_test.csv sample_pipeline/ data_check_pipeline.yml # configuration for the pipeline data/ my_schema.some_table.csv # data for a table data2/ some_data.csv # other data some_checks/ # folder with CSV checks check1.sql check1.csl ... run_this.sql # a SQL file that will be executed cleanup.sql other_pipeline/ # you can have multiple pipelines that will run in parallel data_check_pipeline.yml ... The file sample_pipeline/data_check_pipeline.yml can look like this: steps: # this will truncate the table my_schema.some_table and load it with the data from data/my_schema.some_table.csv - load: data # this will execute the SQL statement in run_this.sql - sql_file: run_this.sql # this will append the data from data2/some_data.csv to my_schema.other_table - load_table: file: data2/some_data.csv table: my_schema.other_table load_mode: append # this will run a python script and pass the connection name - cmd: \"python3 /path/to/my_pipeline.py --connection {{CONNECTION}}\" # this will run the CSV checks in the some_checks folder - check: some_checks - always_run: - sql_file: run_this_always.sql Pipeline checks and simple CSV checks can coexist in a project. Pipeline configuration data_check_pipeline.yml is a YAML file with steps as its main element that contains a list of steps. steps steps is a list of steps that are executed in the pipeline sequentially. If any of the steps fail the pipeline will fail and the following steps will not be executed. Most steps have a list of files as a parameter. If you only need a single file/path, you can usually use a short form: step_name: path . check check will run CSV checks in a given folder or from a single file. It is like running data_check with this parameter. All checks will be performed in parallel. Short form: - check: some_checks Long form: - check: files: - some_checks - some/other/checks.sql You can also omit files : - check: - some_checks - some/other/checks.sql load_table load_table is like calling data_check load --table ... . This will load a CSV file into a table. - load_table: file: check/date_test.csv table: temp.date_test load_mode: append You can omit load_mode . Then the default mode truncate will be used. load load is like calling data_check load ... without --table . This will load one or more tables from CSV files and infer the table name from the file name. Like with data_check load the path before the filename has no impact on the inferred table name, only the file name itself. Short form: - load: some_path/schema.table_name.csv Long form: - load: files: - some_path - some/other/path/schema.other_table.csv load_mode: append You can omit load_mode . Then the default mode truncate will be used. You can also omit files : - load: - some_path - some/other/path/schema.other_table.csv sql_files sql_files is like calling data_check sql --files ... . This will run a SQL file or all SQL files in a folder against the configured database. All SQL files are executed in parallel. If you need to execute a file after another file, you need to call sql_files twice. sql_file is an alias for sql_files . Short form: - sql_files: some_file.sql Using the alias: - sql_file: some_file.sql Long form: - sql_files: files: - some_file.sql - some_path You can also omit files : - sql_files: - some_file.sql - some_path sql sql is like calling data_check sql ... . This will execute a SQL statement given as the parameter. If the SQL is a query, the result will be printed as CSV. Short form: - sql: select 1 as a, 'b' as t Long form: - sql: query: select 1 as a, 'b' as t With output to write a CSV file: - sql: query: select 1 as a, 'b' as t output: result.csv output is relative to the pipeline path, unless an absolute path is specified, for example '{{PROJECT_PATH}}/result.csv'. cmd cmd will call any script or program. The commands will be executed sequentially. The optional print parameter can disable console output of the command. Short form: - cmd: echo \"test\" Long form: - cmd: commands: - echo \"test\" - script/to/start/pipeline.sh print: false With print: false no output is printed from the commands. You can also omit commands : - cmd: - echo \"test\" - script/to/start/pipeline.sh always_run always_run is a container for other steps. These steps will always be executed, even if any other step fail. If always_run is between other steps, it will be executed in order. Example: steps: - sql_file: might_fail.sql - always_run: - sql_file: run_after_failing.sql - cmd: some_script.sh - cmd: other_script.sh - always_run: - sql_file: finish.sql In this example, if might_fail.sql fails, run_after_failing.sql , some_script.sh and finish.sql will be run in this order. If might_fail.sql does not fail, other_script.sh is executed after run_after_failing.sql and some_script.sh . finish.sql will then run at the end (even when other_script.sh fails). nested pipelines Pipelines can be nested inside other pipelines. Passing a folder with a data_check_pipeline.yml to check will run the pipeline: - check: - some_checks - folder_with_a_pipeline Parameters in pipelines You can use some predefined parameters in a pipeline definition: CONNECTION: The name of the connection used for this run. CONNECTION_STRING: The connection string as defined in data_check.yml used for the connection. PROJECT_PATH: The path of the data_check project (the folder containing data_check.yml ). PIPELINE_PATH: The absolute path to the pipeline (the folder containing data_check_pipeline.yml ). PIPELINE_NAME: The name of the folder containing data_check_pipeline.yml . Generating pipeline checks Like generating expectation files you can also run data_check gen for a pipeline. In this mode the pipeline is executed, but each check step will generate the CSV files instead of running the actual checks. Adding --force will overwrite existing CSV files.","title":"Pipelines"},{"location":"pipelines/#pipelines","text":"If data_check finds a file named data_check_pipeline.yml in a folder, it will treat this folder as a pipeline check. Instead of running CSV checks it will execute the steps in the YAML file.","title":"Pipelines"},{"location":"pipelines/#example","text":"Example project with a pipeline: data_check.yml checks/ some_test.sql # this test will run in parallel to the pipeline test some_test.csv sample_pipeline/ data_check_pipeline.yml # configuration for the pipeline data/ my_schema.some_table.csv # data for a table data2/ some_data.csv # other data some_checks/ # folder with CSV checks check1.sql check1.csl ... run_this.sql # a SQL file that will be executed cleanup.sql other_pipeline/ # you can have multiple pipelines that will run in parallel data_check_pipeline.yml ... The file sample_pipeline/data_check_pipeline.yml can look like this: steps: # this will truncate the table my_schema.some_table and load it with the data from data/my_schema.some_table.csv - load: data # this will execute the SQL statement in run_this.sql - sql_file: run_this.sql # this will append the data from data2/some_data.csv to my_schema.other_table - load_table: file: data2/some_data.csv table: my_schema.other_table load_mode: append # this will run a python script and pass the connection name - cmd: \"python3 /path/to/my_pipeline.py --connection {{CONNECTION}}\" # this will run the CSV checks in the some_checks folder - check: some_checks - always_run: - sql_file: run_this_always.sql Pipeline checks and simple CSV checks can coexist in a project.","title":"Example"},{"location":"pipelines/#pipeline-configuration","text":"data_check_pipeline.yml is a YAML file with steps as its main element that contains a list of steps.","title":"Pipeline configuration"},{"location":"pipelines/#steps","text":"steps is a list of steps that are executed in the pipeline sequentially. If any of the steps fail the pipeline will fail and the following steps will not be executed. Most steps have a list of files as a parameter. If you only need a single file/path, you can usually use a short form: step_name: path .","title":"steps"},{"location":"pipelines/#check","text":"check will run CSV checks in a given folder or from a single file. It is like running data_check with this parameter. All checks will be performed in parallel. Short form: - check: some_checks Long form: - check: files: - some_checks - some/other/checks.sql You can also omit files : - check: - some_checks - some/other/checks.sql","title":"check"},{"location":"pipelines/#load_table","text":"load_table is like calling data_check load --table ... . This will load a CSV file into a table. - load_table: file: check/date_test.csv table: temp.date_test load_mode: append You can omit load_mode . Then the default mode truncate will be used.","title":"load_table"},{"location":"pipelines/#load","text":"load is like calling data_check load ... without --table . This will load one or more tables from CSV files and infer the table name from the file name. Like with data_check load the path before the filename has no impact on the inferred table name, only the file name itself. Short form: - load: some_path/schema.table_name.csv Long form: - load: files: - some_path - some/other/path/schema.other_table.csv load_mode: append You can omit load_mode . Then the default mode truncate will be used. You can also omit files : - load: - some_path - some/other/path/schema.other_table.csv","title":"load"},{"location":"pipelines/#sql_files","text":"sql_files is like calling data_check sql --files ... . This will run a SQL file or all SQL files in a folder against the configured database. All SQL files are executed in parallel. If you need to execute a file after another file, you need to call sql_files twice. sql_file is an alias for sql_files . Short form: - sql_files: some_file.sql Using the alias: - sql_file: some_file.sql Long form: - sql_files: files: - some_file.sql - some_path You can also omit files : - sql_files: - some_file.sql - some_path","title":"sql_files"},{"location":"pipelines/#sql","text":"sql is like calling data_check sql ... . This will execute a SQL statement given as the parameter. If the SQL is a query, the result will be printed as CSV. Short form: - sql: select 1 as a, 'b' as t Long form: - sql: query: select 1 as a, 'b' as t With output to write a CSV file: - sql: query: select 1 as a, 'b' as t output: result.csv output is relative to the pipeline path, unless an absolute path is specified, for example '{{PROJECT_PATH}}/result.csv'.","title":"sql"},{"location":"pipelines/#cmd","text":"cmd will call any script or program. The commands will be executed sequentially. The optional print parameter can disable console output of the command. Short form: - cmd: echo \"test\" Long form: - cmd: commands: - echo \"test\" - script/to/start/pipeline.sh print: false With print: false no output is printed from the commands. You can also omit commands : - cmd: - echo \"test\" - script/to/start/pipeline.sh","title":"cmd"},{"location":"pipelines/#always_run","text":"always_run is a container for other steps. These steps will always be executed, even if any other step fail. If always_run is between other steps, it will be executed in order. Example: steps: - sql_file: might_fail.sql - always_run: - sql_file: run_after_failing.sql - cmd: some_script.sh - cmd: other_script.sh - always_run: - sql_file: finish.sql In this example, if might_fail.sql fails, run_after_failing.sql , some_script.sh and finish.sql will be run in this order. If might_fail.sql does not fail, other_script.sh is executed after run_after_failing.sql and some_script.sh . finish.sql will then run at the end (even when other_script.sh fails).","title":"always_run"},{"location":"pipelines/#nested-pipelines","text":"Pipelines can be nested inside other pipelines. Passing a folder with a data_check_pipeline.yml to check will run the pipeline: - check: - some_checks - folder_with_a_pipeline","title":"nested pipelines"},{"location":"pipelines/#parameters-in-pipelines","text":"You can use some predefined parameters in a pipeline definition: CONNECTION: The name of the connection used for this run. CONNECTION_STRING: The connection string as defined in data_check.yml used for the connection. PROJECT_PATH: The path of the data_check project (the folder containing data_check.yml ). PIPELINE_PATH: The absolute path to the pipeline (the folder containing data_check_pipeline.yml ). PIPELINE_NAME: The name of the folder containing data_check_pipeline.yml .","title":"Parameters in pipelines"},{"location":"pipelines/#generating-pipeline-checks","text":"Like generating expectation files you can also run data_check gen for a pipeline. In this mode the pipeline is executed, but each check step will generate the CSV files instead of running the actual checks. Adding --force will overwrite existing CSV files.","title":"Generating pipeline checks"},{"location":"test_data/","text":"Test data data_check fake fake_config.yml is used to generate test data for a table from a configuration file. The data type for each column is deferred from the table and can be changed in the configuration. See Usage for command line options. Test data generation is done using Faker . Example The minimal configuration only names the table for test data generation: table: main.simple_table When running data_check fake fake_config.yml the column definition from the table is read from the database and a CSV file main.simple_table.csv is generated with some data for the table (100 rows by default). The CSV file can be used to load the data back into the table. A more complete configuration looks like this and is described in the following: table: main.simple_table business_key: # the key that should not change between iterations; must not be null - bkey rows: 200 # how many rows to generate iterations: # generate data with same business_key with some variation count: 5 # how many iterations to generate columns: bkey: faker: iban date_col: add_values: # also use these value - 1900-01-01 - 9999-12-31 col2: faker: name # faker provider method, if not correctly inferred col3: from_query: select colx from main.other_table # use values from the query next: inc # \"algorithm\" for next iteration col4: next: random values: # use these values randomly - 1 - 2 - null Test data configuration The configuration is a YAML file for a single table. The top level elements are: table table tells data_check for which table to generate the data. Example: table: main.simple_table business_key business_key is a list of columns that are unique and do not change between iterations. Example: business_key: - column_1 - column_2 iterations iterations configures how many iterations will be generated. Each iteration is a single CSV file with the same business keys. Each column can define a next algorithm how the data should change between iterations. Example: iterations: count: 5 columns columns defines a configuration for each column. If a column is not listed here, a default configuration will be used based on the data type. Example: columns: column_1: faker: iban column_2: from_query: select colx from main.other_table column configuration Each column can have multiple configurations: faker faker defines the provider used to generate the data. If not given, a default provider is used based on the data type of the column. faker: name data type default provider decimal pydecimal varchar pystr date date_between datetime date_time_between faker_args: faker_args defines a map or arguments that are passed to the provider . Each provider can define different arguments. Example: faker: date_between faker_args: start_date: 1900-01-01 end_date: 2030-12-31 from_query from_query defines a SQL query. The values of the query are used randomly to generate the data. If from_query is given, faker is ignored. from_query: select column from other_table values value defines a list of values that are used randomly to generate the data. If values is given, from_query and faker are ignored. values: - 1 - 2 - null add_values add_values is used to add some specific values that are additionally used to generate the data. add_values can be used with faker , from_query and values . Each value in add_values has the same probability to occur as all the other generated values combined. add_values: - 9999-12-31 - 1900-01-01 next next defines an algorithm for the iterator. If next is not given then the column is not changed between iterations. next: inc Possible values for next : inc : increment by 1 (day for date) dec : decrement by 1 (day for date) random : generate a random value from the configured faker/values","title":"Test data"},{"location":"test_data/#test-data","text":"data_check fake fake_config.yml is used to generate test data for a table from a configuration file. The data type for each column is deferred from the table and can be changed in the configuration. See Usage for command line options. Test data generation is done using Faker .","title":"Test data"},{"location":"test_data/#example","text":"The minimal configuration only names the table for test data generation: table: main.simple_table When running data_check fake fake_config.yml the column definition from the table is read from the database and a CSV file main.simple_table.csv is generated with some data for the table (100 rows by default). The CSV file can be used to load the data back into the table. A more complete configuration looks like this and is described in the following: table: main.simple_table business_key: # the key that should not change between iterations; must not be null - bkey rows: 200 # how many rows to generate iterations: # generate data with same business_key with some variation count: 5 # how many iterations to generate columns: bkey: faker: iban date_col: add_values: # also use these value - 1900-01-01 - 9999-12-31 col2: faker: name # faker provider method, if not correctly inferred col3: from_query: select colx from main.other_table # use values from the query next: inc # \"algorithm\" for next iteration col4: next: random values: # use these values randomly - 1 - 2 - null","title":"Example"},{"location":"test_data/#test-data-configuration","text":"The configuration is a YAML file for a single table. The top level elements are:","title":"Test data configuration"},{"location":"test_data/#table","text":"table tells data_check for which table to generate the data. Example: table: main.simple_table","title":"table"},{"location":"test_data/#business_key","text":"business_key is a list of columns that are unique and do not change between iterations. Example: business_key: - column_1 - column_2","title":"business_key"},{"location":"test_data/#iterations","text":"iterations configures how many iterations will be generated. Each iteration is a single CSV file with the same business keys. Each column can define a next algorithm how the data should change between iterations. Example: iterations: count: 5","title":"iterations"},{"location":"test_data/#columns","text":"columns defines a configuration for each column. If a column is not listed here, a default configuration will be used based on the data type. Example: columns: column_1: faker: iban column_2: from_query: select colx from main.other_table","title":"columns"},{"location":"test_data/#column-configuration","text":"Each column can have multiple configurations:","title":"column configuration"},{"location":"test_data/#faker","text":"faker defines the provider used to generate the data. If not given, a default provider is used based on the data type of the column. faker: name data type default provider decimal pydecimal varchar pystr date date_between datetime date_time_between","title":"faker"},{"location":"test_data/#faker_args","text":"faker_args defines a map or arguments that are passed to the provider . Each provider can define different arguments. Example: faker: date_between faker_args: start_date: 1900-01-01 end_date: 2030-12-31","title":"faker_args:"},{"location":"test_data/#from_query","text":"from_query defines a SQL query. The values of the query are used randomly to generate the data. If from_query is given, faker is ignored. from_query: select column from other_table","title":"from_query"},{"location":"test_data/#values","text":"value defines a list of values that are used randomly to generate the data. If values is given, from_query and faker are ignored. values: - 1 - 2 - null","title":"values"},{"location":"test_data/#add_values","text":"add_values is used to add some specific values that are additionally used to generate the data. add_values can be used with faker , from_query and values . Each value in add_values has the same probability to occur as all the other generated values combined. add_values: - 9999-12-31 - 1900-01-01","title":"add_values"},{"location":"test_data/#next","text":"next defines an algorithm for the iterator. If next is not given then the column is not changed between iterations. next: inc Possible values for next : inc : increment by 1 (day for date) dec : decrement by 1 (day for date) random : generate a random value from the configured faker/values","title":"next"},{"location":"usage/","text":"Usage Commands data_check run - Run checks (default command) . data_check fake - Generate test data . data_check gen - Generate CSV files from query files . data_check load - Load data from files into tables . data_check ping - Tries to connect to the database . data_check sql - Run SQL statements . Common options Common options can be used with any command. -c/--connection CONNECTION - Use another connection than the default. -n/--workers WORKERS - Use WORKERS threads to run the queries (default: 4). --config CONFIG - Config file to use (default: data_check.yml). --quiet - Do not print any output. --verbose - Print verbose output. --traceback - Print traceback output for debugging. --log LOGFILE - Write output to a log file. --version - Show the version and exit. --help - Show this message and exit. run run executes checks against a database. It is the default command and can be omitted. Options --print - Print failed results data. --print-format FORMAT - Format for printing failed results (csv (default), pandas, json). --print-json - Shortcut for \"--print --print-format json\". --diff - Print only the different columns for failed results. Use with --print. Examples data_check run - Run data_check against the default connection in the checks folder. data_check - Same as data_check run data_check run some_folder - Run data_check against the default connection in the some_folder folder. data_check run some_folder/some_file.sql - Run data_check against the default connection for a single test. data_check run --print - Run data_check against the default connection in the checks folder and prints all failed result data. data_check run --print --diff some_folder - Run data_check against the default connection in the some_folder folder and prints only the different columns for failed results. fake fake generates test data for existing tables and writes it into CSV files. The configuration file is described in Test data . Options -o/--output PATH - Output path for the CSV file. --force - Overwrite existing files. Examples data_check fake fake_config.yml - Generates test data as defined in fake_config.yml and writes it into a CSV file with the same name as the table name. data_check fake fake_config.yml --output fake.csv - Generates test data as defined in fake_config.yml and writes it into fake.csv . data_check fake fake_config.yml --force - Generates test data as defined in fake_config.yml , overwrites existing CSV files. data_check fake fake_config.yml fake_config2.yml - Generates test data for both config files. gen gen generates the expectation files (CSV). Options --force - Overwrite existing files. Examples data_check gen - Generates all missing expectation files in the checks folder. data_check gen --force some_folder - Generates and overwrites all expectation files in the some_folder folder. load load is used to load data from files (CSV or Excel) into database tables. By default it will load into a table with the same name as the file and truncate the table before loading. If the table does not exists, it will create the table with a column definition guessed from the file content. Options --table - Table name to load data into --mode - How to load the table: truncate (default), append or replace. Examples data_check load some_path/schema.table.csv - Truncates the table schema.table and loads the csv from some_path/schema.table.csv into the table. data_check load --table schema.other_table some_path/schema.table.csv - Same as above but uses the table schema.other_table. Only works for a single CSV/Excel file. data_check load --mode append some_path/schema.table.csv - Loads the csv from some_path/schema.table.csv into the table, appends only without touching the existing data in the table. data_check load --mode replace some_path/schema.table.csv - Drops and recreates the table schema.table and loads the csv from some_path/schema.table.csv into the table. data_check load some_path/schema.table.csv other_path/schema2.other_table.csv - Loads multiple tables from multiple files. data_check load some_path - Loads all CSV/Excel files from some_path each into the same table as the file name. ping ping tries to connect to the database. The exit code is 0 if it successful, otherwise it is 1. ping doesn't have any special options, except the common ones. Examples data_check ping - Tries to connect to the default database. data_check ping --connection test2 - Tries to connect to the database with the connection test2 . data_check ping --quiet - Tries to connect to the default database. Doesn't print anything. sql sql runs any SQL query/command against the database. The query can be passed as an argument or from a file. The result of the query can be written into a CSV file. Options --file, --file - Run any SQL script in a list of SQL files. -o/--output PATH - Output path for the result. -W/--write-check PATH - Create a check from the sql statement (SQL and CSV file). Examples data_check sql \"select * from some.table\" - Runs the given query against the default database connection. data_check sql --connection test2 \"select * from some.table\" - Runs the given query against the database connection for test2 . data_check sql --file some/file.sql - Runs the query in the file against the default database connection. data_check sql --files some/path - Runs the queries from all SQL files in some/path against the default database connection. The queries are run in parallel, the order is random. data_check sql --workers 1 --files some/path - Runs the queries from all SQL files in some/path against the default database connection. The queries are run sequentially ordered by the path/file name. Exit codes Possible exit codes: Exit code 0: All tests run successfully. Exit code 1: At least one test failed. Logging --log will write the output to a log file and to the console. With --quiet the output will only be printed to the log file. If the file exists, the output will be appended. You can also set the log file in data_check.yml: log: logfile.txt Failed results and tracebacks are always written to the log file, even when the parameters --print and --traceback are not used. Loading data into tables Sometimes you need to populate tables with some data before running pipeline tests. With data_check you can use CSV or Excel files to load data into tables. The CSV format is the same as used for testing. The header in the CSV file or the first row in the Excel file must match the columns in the table. Additionally, Excel cells can use its native date/datetime format. If the table doesn't exist, it will be created. The schema and table names are always case-insensitive, as long as the database supports it. Otherwise, they are lowercased. Loading data into a single table To load data from some CSV or Excel file into a table, you can use data_check load path/to/some_file.[csv/xlsx] --table schema.table_name . This will truncate the table and load the content of the file into the table. You can specify different load modes if you do not want to truncate the table. Loading data into multiple tables You can use multiple CSV and Excel files or a whole folder to load the data into tables. The table name will be derived from the file name. data_check load path/schema.table_1.csv - this will load the data from the CSV file into the table schema.table_1 data_check load path/schema.table_2.xlsx - this will load the data from the Excel file into the table schema.table_2 data_check load path/to/some_folder - this will load the data from all CSV and Excel files in this folder into tables matching the file names. The path will be searched recursively for CSV and Excel files. The folder structure doesn't matter when matching the table names, only the file name matters. Load modes There are multiple modes that control the data loading: truncate - truncate the table before loading the data. replace - drop the table and recreate it based on the columns in the CSV or Excel file. append - do not touch the table before loading the data. By default, the tables will be truncated before loading the data. You can use the other modes with --load-mode : data_check load file.csv --table table_name --mode replace data_check load path/to/tables_folder --mode append CSV and data types When loading data from CSV files, data_check (or more precisely: pandas ) will infer the data types from the file. When loading the data into the table, the database will usually implicitly convert the data types. This works good for simple data types like strings and numbers. If you need to load date types (or timestamps) and the table has a date column, data_check will try to convert these columns in the CSV file into a datetime. This doesn't work when using --load-mode replace since the table will be dropped before it can be analyzed. This will probably result in a varchar column instead of date. Use ISO 8601 for dates, like for [CSV checks]((csv_checks.md#csv-format) Executing arbitrary SQL code You can run any SQL file against the database by using the --sql-files command: data_check sql --files sql_file.sql other_file.sql or a whole folder recursively: data_check sql --files some/folder/with/sql_files All files are run in parallel. If you have dependencies between the files, data_check must be called sequentially for each file. Multiple statements in a SQL file usually must be inside an anonymous block. MySQL doesn't support this however. --file is an alias for --files . Use it to indicate, that you only want to run a single file. You can also run a SQL statement directly from the command line: data_check sql \"select * from tableX\" This will execute the query and, if it is a query, print the result as CSV. You can also write the result into a file: data_check sql \"select * from tableX\" --output some_file.csv You can use both templates and lookups with sql . Generating a CSV check from a SQL statement You can use sql to create a CSV check: data_check sql \"select * from tableX\" --write-check some_check.sql This writes the SQL statement into some_check.sql and the result as CSV into some_check.csv .","title":"Usage"},{"location":"usage/#usage","text":"","title":"Usage"},{"location":"usage/#commands","text":"data_check run - Run checks (default command) . data_check fake - Generate test data . data_check gen - Generate CSV files from query files . data_check load - Load data from files into tables . data_check ping - Tries to connect to the database . data_check sql - Run SQL statements .","title":"Commands"},{"location":"usage/#common-options","text":"Common options can be used with any command. -c/--connection CONNECTION - Use another connection than the default. -n/--workers WORKERS - Use WORKERS threads to run the queries (default: 4). --config CONFIG - Config file to use (default: data_check.yml). --quiet - Do not print any output. --verbose - Print verbose output. --traceback - Print traceback output for debugging. --log LOGFILE - Write output to a log file. --version - Show the version and exit. --help - Show this message and exit.","title":"Common options"},{"location":"usage/#run","text":"run executes checks against a database. It is the default command and can be omitted.","title":"run"},{"location":"usage/#options","text":"--print - Print failed results data. --print-format FORMAT - Format for printing failed results (csv (default), pandas, json). --print-json - Shortcut for \"--print --print-format json\". --diff - Print only the different columns for failed results. Use with --print.","title":"Options"},{"location":"usage/#examples","text":"data_check run - Run data_check against the default connection in the checks folder. data_check - Same as data_check run data_check run some_folder - Run data_check against the default connection in the some_folder folder. data_check run some_folder/some_file.sql - Run data_check against the default connection for a single test. data_check run --print - Run data_check against the default connection in the checks folder and prints all failed result data. data_check run --print --diff some_folder - Run data_check against the default connection in the some_folder folder and prints only the different columns for failed results.","title":"Examples"},{"location":"usage/#fake","text":"fake generates test data for existing tables and writes it into CSV files. The configuration file is described in Test data .","title":"fake"},{"location":"usage/#options_1","text":"-o/--output PATH - Output path for the CSV file. --force - Overwrite existing files.","title":"Options"},{"location":"usage/#examples_1","text":"data_check fake fake_config.yml - Generates test data as defined in fake_config.yml and writes it into a CSV file with the same name as the table name. data_check fake fake_config.yml --output fake.csv - Generates test data as defined in fake_config.yml and writes it into fake.csv . data_check fake fake_config.yml --force - Generates test data as defined in fake_config.yml , overwrites existing CSV files. data_check fake fake_config.yml fake_config2.yml - Generates test data for both config files.","title":"Examples"},{"location":"usage/#gen","text":"gen generates the expectation files (CSV).","title":"gen"},{"location":"usage/#options_2","text":"--force - Overwrite existing files.","title":"Options"},{"location":"usage/#examples_2","text":"data_check gen - Generates all missing expectation files in the checks folder. data_check gen --force some_folder - Generates and overwrites all expectation files in the some_folder folder.","title":"Examples"},{"location":"usage/#load","text":"load is used to load data from files (CSV or Excel) into database tables. By default it will load into a table with the same name as the file and truncate the table before loading. If the table does not exists, it will create the table with a column definition guessed from the file content.","title":"load"},{"location":"usage/#options_3","text":"--table - Table name to load data into --mode - How to load the table: truncate (default), append or replace.","title":"Options"},{"location":"usage/#examples_3","text":"data_check load some_path/schema.table.csv - Truncates the table schema.table and loads the csv from some_path/schema.table.csv into the table. data_check load --table schema.other_table some_path/schema.table.csv - Same as above but uses the table schema.other_table. Only works for a single CSV/Excel file. data_check load --mode append some_path/schema.table.csv - Loads the csv from some_path/schema.table.csv into the table, appends only without touching the existing data in the table. data_check load --mode replace some_path/schema.table.csv - Drops and recreates the table schema.table and loads the csv from some_path/schema.table.csv into the table. data_check load some_path/schema.table.csv other_path/schema2.other_table.csv - Loads multiple tables from multiple files. data_check load some_path - Loads all CSV/Excel files from some_path each into the same table as the file name.","title":"Examples"},{"location":"usage/#ping","text":"ping tries to connect to the database. The exit code is 0 if it successful, otherwise it is 1. ping doesn't have any special options, except the common ones.","title":"ping"},{"location":"usage/#examples_4","text":"data_check ping - Tries to connect to the default database. data_check ping --connection test2 - Tries to connect to the database with the connection test2 . data_check ping --quiet - Tries to connect to the default database. Doesn't print anything.","title":"Examples"},{"location":"usage/#sql","text":"sql runs any SQL query/command against the database. The query can be passed as an argument or from a file. The result of the query can be written into a CSV file.","title":"sql"},{"location":"usage/#options_4","text":"--file, --file - Run any SQL script in a list of SQL files. -o/--output PATH - Output path for the result. -W/--write-check PATH - Create a check from the sql statement (SQL and CSV file).","title":"Options"},{"location":"usage/#examples_5","text":"data_check sql \"select * from some.table\" - Runs the given query against the default database connection. data_check sql --connection test2 \"select * from some.table\" - Runs the given query against the database connection for test2 . data_check sql --file some/file.sql - Runs the query in the file against the default database connection. data_check sql --files some/path - Runs the queries from all SQL files in some/path against the default database connection. The queries are run in parallel, the order is random. data_check sql --workers 1 --files some/path - Runs the queries from all SQL files in some/path against the default database connection. The queries are run sequentially ordered by the path/file name.","title":"Examples"},{"location":"usage/#exit-codes","text":"Possible exit codes: Exit code 0: All tests run successfully. Exit code 1: At least one test failed.","title":"Exit codes"},{"location":"usage/#logging","text":"--log will write the output to a log file and to the console. With --quiet the output will only be printed to the log file. If the file exists, the output will be appended. You can also set the log file in data_check.yml: log: logfile.txt Failed results and tracebacks are always written to the log file, even when the parameters --print and --traceback are not used.","title":"Logging"},{"location":"usage/#loading-data-into-tables","text":"Sometimes you need to populate tables with some data before running pipeline tests. With data_check you can use CSV or Excel files to load data into tables. The CSV format is the same as used for testing. The header in the CSV file or the first row in the Excel file must match the columns in the table. Additionally, Excel cells can use its native date/datetime format. If the table doesn't exist, it will be created. The schema and table names are always case-insensitive, as long as the database supports it. Otherwise, they are lowercased.","title":"Loading data into tables"},{"location":"usage/#loading-data-into-a-single-table","text":"To load data from some CSV or Excel file into a table, you can use data_check load path/to/some_file.[csv/xlsx] --table schema.table_name . This will truncate the table and load the content of the file into the table. You can specify different load modes if you do not want to truncate the table.","title":"Loading data into a single table"},{"location":"usage/#loading-data-into-multiple-tables","text":"You can use multiple CSV and Excel files or a whole folder to load the data into tables. The table name will be derived from the file name. data_check load path/schema.table_1.csv - this will load the data from the CSV file into the table schema.table_1 data_check load path/schema.table_2.xlsx - this will load the data from the Excel file into the table schema.table_2 data_check load path/to/some_folder - this will load the data from all CSV and Excel files in this folder into tables matching the file names. The path will be searched recursively for CSV and Excel files. The folder structure doesn't matter when matching the table names, only the file name matters.","title":"Loading data into multiple tables"},{"location":"usage/#load-modes","text":"There are multiple modes that control the data loading: truncate - truncate the table before loading the data. replace - drop the table and recreate it based on the columns in the CSV or Excel file. append - do not touch the table before loading the data. By default, the tables will be truncated before loading the data. You can use the other modes with --load-mode : data_check load file.csv --table table_name --mode replace data_check load path/to/tables_folder --mode append","title":"Load modes"},{"location":"usage/#csv-and-data-types","text":"When loading data from CSV files, data_check (or more precisely: pandas ) will infer the data types from the file. When loading the data into the table, the database will usually implicitly convert the data types. This works good for simple data types like strings and numbers. If you need to load date types (or timestamps) and the table has a date column, data_check will try to convert these columns in the CSV file into a datetime. This doesn't work when using --load-mode replace since the table will be dropped before it can be analyzed. This will probably result in a varchar column instead of date. Use ISO 8601 for dates, like for [CSV checks]((csv_checks.md#csv-format)","title":"CSV and data types"},{"location":"usage/#executing-arbitrary-sql-code","text":"You can run any SQL file against the database by using the --sql-files command: data_check sql --files sql_file.sql other_file.sql or a whole folder recursively: data_check sql --files some/folder/with/sql_files All files are run in parallel. If you have dependencies between the files, data_check must be called sequentially for each file. Multiple statements in a SQL file usually must be inside an anonymous block. MySQL doesn't support this however. --file is an alias for --files . Use it to indicate, that you only want to run a single file. You can also run a SQL statement directly from the command line: data_check sql \"select * from tableX\" This will execute the query and, if it is a query, print the result as CSV. You can also write the result into a file: data_check sql \"select * from tableX\" --output some_file.csv You can use both templates and lookups with sql .","title":"Executing arbitrary SQL code"},{"location":"usage/#generating-a-csv-check-from-a-sql-statement","text":"You can use sql to create a CSV check: data_check sql \"select * from tableX\" --write-check some_check.sql This writes the SQL statement into some_check.sql and the result as CSV into some_check.csv .","title":"Generating a CSV check from a SQL statement"}]}