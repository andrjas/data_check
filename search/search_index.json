{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to data_check data_check is a simple data validation tool. Write SQL queries and CSV files with the expected result sets and data_check will test the result sets against the queries. data_check should work with any database that works with SQLAlchemy . Currently data_check is tested against PostgreSQL, MySQL, SQLite, Oracle and Microsoft SQL Server. Why? data_check tries to solve a simple problem in data domains: during development you create multiple SQL queries to validate your data and compare the result manually. With data_check you can organize your data tests and run them automatically, for example in a CI pipeline. How to get started First install data_check. You can then try data_check with the sample project . To create a new project folder write a data_check.yml for the configuration and put your test files in the checks folder. Project layout data_check has a simple layout for projects: a single configuration file and a folder with the test files. You can also organize the test files in subfolders. data_check.yml # The configuration file checks/ # Default folder for data tests some_test.sql # SQL file with the query to run against the database some_test.csv # CSV file with the expected result subfolder/ # Tests can be nested in subfolders Configuration data_check uses the data_check.yml file in the current folder for configuration. This is a simple YAML file: default_connection: con connections: con: connection-string Under connections you can put multiple connection strings with names. default_connection is the connection name that data_check uses when no additional arguments are given. You can also use environment variables in the connection string to store the credentials outside data_check.yml (e.g. postgresql://postgres:${POSTGRES_PWD}@db:5432 ). Test files The test files are organized in the checks folder and its subfolders. data_check will run the queries for each SQL file in these folders and compare the results with the corresponding CSV files. The CSV file must be named exactly like the SQL file, only with different file endings. Instead of writing the CSV files manually, you can also generate them from the SQL files. How it works data_check uses pandas and SQLAlchemy internally. The SQL files are executed and converted to DataFrames. The CSV files are also parsed to DataFrames. Both DataFrames are then merged with a full outer join. If some rows differ, the test is considered as failed.","title":"Home"},{"location":"#welcome-to-data_check","text":"data_check is a simple data validation tool. Write SQL queries and CSV files with the expected result sets and data_check will test the result sets against the queries. data_check should work with any database that works with SQLAlchemy . Currently data_check is tested against PostgreSQL, MySQL, SQLite, Oracle and Microsoft SQL Server.","title":"Welcome to data_check"},{"location":"#why","text":"data_check tries to solve a simple problem in data domains: during development you create multiple SQL queries to validate your data and compare the result manually. With data_check you can organize your data tests and run them automatically, for example in a CI pipeline.","title":"Why?"},{"location":"#how-to-get-started","text":"First install data_check. You can then try data_check with the sample project . To create a new project folder write a data_check.yml for the configuration and put your test files in the checks folder.","title":"How to get started"},{"location":"#project-layout","text":"data_check has a simple layout for projects: a single configuration file and a folder with the test files. You can also organize the test files in subfolders. data_check.yml # The configuration file checks/ # Default folder for data tests some_test.sql # SQL file with the query to run against the database some_test.csv # CSV file with the expected result subfolder/ # Tests can be nested in subfolders","title":"Project layout"},{"location":"#configuration","text":"data_check uses the data_check.yml file in the current folder for configuration. This is a simple YAML file: default_connection: con connections: con: connection-string Under connections you can put multiple connection strings with names. default_connection is the connection name that data_check uses when no additional arguments are given. You can also use environment variables in the connection string to store the credentials outside data_check.yml (e.g. postgresql://postgres:${POSTGRES_PWD}@db:5432 ).","title":"Configuration"},{"location":"#test-files","text":"The test files are organized in the checks folder and its subfolders. data_check will run the queries for each SQL file in these folders and compare the results with the corresponding CSV files. The CSV file must be named exactly like the SQL file, only with different file endings. Instead of writing the CSV files manually, you can also generate them from the SQL files.","title":"Test files"},{"location":"#how-it-works","text":"data_check uses pandas and SQLAlchemy internally. The SQL files are executed and converted to DataFrames. The CSV files are also parsed to DataFrames. Both DataFrames are then merged with a full outer join. If some rows differ, the test is considered as failed.","title":"How it works"},{"location":"development/","text":"Development poetry must be installed first for development. To set up a development environment initially with poetry: poetry install Later, just activate the virtual environment: poetry shell Please use Black to format the code before committing any change: black data_check Testing data_check has two layers of tests: Unit tests pytest is used for unit testing. There are two types of test for data_check in the test folder: Basic tests for the code and tests against a database. For unit tests an in-memory SQLite database that is integrated into Python is used. Run pytest inside the virtual environment to execute the unit tests. Integration tests Integration tests are using specific databases and run unit tests and data_check test against this database. There are currently four databases used for integration tests: PostgreSQL MySQL Oracle Microsoft SQL Server All integration tests are located in the int_test folder. The tests should be executed from the root folder of this Git repository. Run ./int_test/<db>/int_test.sh to execute a test against a database, e.g. ./int_test/postgres/int_test.sh The scripts are using Docker and Docker Compose to set up a Docker container with the database and another with data_check and all required database drivers. First the same unit tests as mentioned above are run against the database. Then data_check is executed. Each database has its own checks folder in int_test to adhere to different SQL dialects.","title":"Development"},{"location":"development/#development","text":"poetry must be installed first for development. To set up a development environment initially with poetry: poetry install Later, just activate the virtual environment: poetry shell Please use Black to format the code before committing any change: black data_check","title":"Development"},{"location":"development/#testing","text":"data_check has two layers of tests:","title":"Testing"},{"location":"development/#unit-tests","text":"pytest is used for unit testing. There are two types of test for data_check in the test folder: Basic tests for the code and tests against a database. For unit tests an in-memory SQLite database that is integrated into Python is used. Run pytest inside the virtual environment to execute the unit tests.","title":"Unit tests"},{"location":"development/#integration-tests","text":"Integration tests are using specific databases and run unit tests and data_check test against this database. There are currently four databases used for integration tests: PostgreSQL MySQL Oracle Microsoft SQL Server All integration tests are located in the int_test folder. The tests should be executed from the root folder of this Git repository. Run ./int_test/<db>/int_test.sh to execute a test against a database, e.g. ./int_test/postgres/int_test.sh The scripts are using Docker and Docker Compose to set up a Docker container with the database and another with data_check and all required database drivers. First the same unit tests as mentioned above are run against the database. Then data_check is executed. Each database has its own checks folder in int_test to adhere to different SQL dialects.","title":"Integration tests"},{"location":"example/","text":"Examples data_check sample project This Git repository is also a sample data_check project. Clone the repository, switch to the folder and run data_check: git clone git@github.com:andrjas/data_check.git cd data_check data_check This will run the tests in the checks folder using the default connection as defined in data_check.yml. The result will tell you which tests passed and which failed: checks/generated/generate_before_running.sql: NO EXPECTED RESULTS FILE checks/failing/invalid.sql: FAILED (with exception in checks/failing/invalid.sql) checks/failing/expected_to_fail.sql: FAILED checks/basic/simple_string.sql: PASSED checks/basic/data_types.sql: PASSED checks/basic/float.sql: PASSED checks/basic/unicode_string.sql: PASSED Tests structure You can structure your test in many ways. You can also mix there structures. By pipeline You can structure your tests to run before/after some data pipeline has run: checks/ pipeline1/ pre/ test1.sql test1.csv ... post/ ... pipeline2/ pre/ ... post/ ... By test execution time In a CI environment you can structure your tests after the expected execution time of the tests. checks/ quick_tests/ ... medium_tests/ ... slow_running_tests/ ... This way you can run quick test, e.g. schema validation, many times during development. Other tests that must process a lot of data can be run less frequently, e.g. in a integration environment. Database URLs These are the connection strings used in the integration test . Other connection strings can be found in the SQLAlchemy documentation . PostgreSQL postgresql://username:password@db_host:5432/db_name Oracle oracle+cx_oracle://username:password@db_host:1521/?service_name=XEPDB1 MySQL/MariaDB mysql+pymysql://username:password@db_host:3306/db_name Microsoft SQL Server mssql+pyodbc://username:password@db_host:1433/db_name?driver=ODBC+Driver+17+for+SQL+Server","title":"Examples"},{"location":"example/#examples","text":"","title":"Examples"},{"location":"example/#data_check-sample-project","text":"This Git repository is also a sample data_check project. Clone the repository, switch to the folder and run data_check: git clone git@github.com:andrjas/data_check.git cd data_check data_check This will run the tests in the checks folder using the default connection as defined in data_check.yml. The result will tell you which tests passed and which failed: checks/generated/generate_before_running.sql: NO EXPECTED RESULTS FILE checks/failing/invalid.sql: FAILED (with exception in checks/failing/invalid.sql) checks/failing/expected_to_fail.sql: FAILED checks/basic/simple_string.sql: PASSED checks/basic/data_types.sql: PASSED checks/basic/float.sql: PASSED checks/basic/unicode_string.sql: PASSED","title":"data_check sample project"},{"location":"example/#tests-structure","text":"You can structure your test in many ways. You can also mix there structures.","title":"Tests structure"},{"location":"example/#by-pipeline","text":"You can structure your tests to run before/after some data pipeline has run: checks/ pipeline1/ pre/ test1.sql test1.csv ... post/ ... pipeline2/ pre/ ... post/ ...","title":"By pipeline"},{"location":"example/#by-test-execution-time","text":"In a CI environment you can structure your tests after the expected execution time of the tests. checks/ quick_tests/ ... medium_tests/ ... slow_running_tests/ ... This way you can run quick test, e.g. schema validation, many times during development. Other tests that must process a lot of data can be run less frequently, e.g. in a integration environment.","title":"By test execution time"},{"location":"example/#database-urls","text":"These are the connection strings used in the integration test . Other connection strings can be found in the SQLAlchemy documentation .","title":"Database URLs"},{"location":"example/#postgresql","text":"postgresql://username:password@db_host:5432/db_name","title":"PostgreSQL"},{"location":"example/#oracle","text":"oracle+cx_oracle://username:password@db_host:1521/?service_name=XEPDB1","title":"Oracle"},{"location":"example/#mysqlmariadb","text":"mysql+pymysql://username:password@db_host:3306/db_name","title":"MySQL/MariaDB"},{"location":"example/#microsoft-sql-server","text":"mssql+pyodbc://username:password@db_host:1433/db_name?driver=ODBC+Driver+17+for+SQL+Server","title":"Microsoft SQL Server"},{"location":"howtos/","text":"HowTos This is a collection of various things you can do with data_check. Use another configuration file data_check --config other_config_file.yml Run checks in multiple folders Run all test in the folders check_folder_1 and check_folder_2 : data_check check_folder_1 check_folder_2","title":"HowTos"},{"location":"howtos/#howtos","text":"This is a collection of various things you can do with data_check.","title":"HowTos"},{"location":"howtos/#use-another-configuration-file","text":"data_check --config other_config_file.yml","title":"Use another configuration file"},{"location":"howtos/#run-checks-in-multiple-folders","text":"Run all test in the folders check_folder_1 and check_folder_2 : data_check check_folder_1 check_folder_2","title":"Run checks in multiple folders"},{"location":"install/","text":"Installing data_check Depending on your system there are many ways to install data_check. Generally the steps are always the same: create a virtual environment activate the virtual environment optionally: install pip install data_check You should then be able to run data_check . To run data_check next time in a new terminal you must just activate the virtual environment. Note: While the tool is called 'data_check' the package that you install is called 'data-check'. With pipx The easiest way to install data_check is to use pipx : pipx install data-check To upgrade data_check via pipx: pipx upgrade data-check With Anaconda/Miniconda Create a new conda environment: conda create --name data_check python=3.8 Activate the environment: conda activate data_check and install pip: conda install pip Install data_check via pip: python3 -m pip install data-check To upgrade data_check in the conda environment: python3 -m pip install data-check --upgrade With venv Create a virtual environment ( <venv> point to a relative or absolute path, e.g. c:\\venvs\\data_check) python3 -m venv <venv> Activate the environment: Bash: source <venv>/bin/activate PowerShell: <venv>\\Scripts\\Activate.ps1 The Python documentation has more options how to enable a virtual environment. Install data_check via pip: python3 -m pip install data-check To upgrade data_check in the virtual environment: python3 -m pip install data-check --upgrade Databases Installing data_check alone will only support SQLite, which is bundled with Python. You need additional drivers for other databases. See https://docs.sqlalchemy.org/en/14/dialects/index.html for all possible drivers and how to install them. With pipx you can install the drivers with pipx inject data-check <drivername> (watch the minus sign in data-check instead of the underscore). In a virtual environment you just activate the environment and run pip install <drivername> . Some drivers need additional dependencies. Here are the drivers used for testing data_check: PostgreSQL psycopg2-binary should work on most systems without any additional dependencies. You can use data-check[postgres] to install data_check directly with psycopg2-binary : e.g. with pipx: pipx install data-check[postgres] MySQL/MariaDB PyMySQL as described in https://pypi.org/project/PyMySQL/ with additional cryptography dependencies. Use pipx install data-check[mysql] to install data_check with PyMySQL[rsa] . Microsoft SQL Server pyodbc needs unixodbc and the development package (unixodbc-dev) on Linux. Additionally you must install the Microsoft ODBC driver for SQL Server on your system. Use pipx install data-check[mssql] to install data_check with pyodbc . Oracle cx_Oracle needs Oracle client libraries to work. https://cx-oracle.readthedocs.io/en/latest/user_guide/installation.html shows how to install them. Use pipx install data-check[oracle] to install data_check with cx_Oracle .","title":"Installation"},{"location":"install/#installing-data_check","text":"Depending on your system there are many ways to install data_check. Generally the steps are always the same: create a virtual environment activate the virtual environment optionally: install pip install data_check You should then be able to run data_check . To run data_check next time in a new terminal you must just activate the virtual environment. Note: While the tool is called 'data_check' the package that you install is called 'data-check'.","title":"Installing data_check"},{"location":"install/#with-pipx","text":"The easiest way to install data_check is to use pipx : pipx install data-check To upgrade data_check via pipx: pipx upgrade data-check","title":"With pipx"},{"location":"install/#with-anacondaminiconda","text":"Create a new conda environment: conda create --name data_check python=3.8 Activate the environment: conda activate data_check and install pip: conda install pip Install data_check via pip: python3 -m pip install data-check To upgrade data_check in the conda environment: python3 -m pip install data-check --upgrade","title":"With Anaconda/Miniconda"},{"location":"install/#with-venv","text":"Create a virtual environment ( <venv> point to a relative or absolute path, e.g. c:\\venvs\\data_check) python3 -m venv <venv> Activate the environment: Bash: source <venv>/bin/activate PowerShell: <venv>\\Scripts\\Activate.ps1 The Python documentation has more options how to enable a virtual environment. Install data_check via pip: python3 -m pip install data-check To upgrade data_check in the virtual environment: python3 -m pip install data-check --upgrade","title":"With venv"},{"location":"install/#databases","text":"Installing data_check alone will only support SQLite, which is bundled with Python. You need additional drivers for other databases. See https://docs.sqlalchemy.org/en/14/dialects/index.html for all possible drivers and how to install them. With pipx you can install the drivers with pipx inject data-check <drivername> (watch the minus sign in data-check instead of the underscore). In a virtual environment you just activate the environment and run pip install <drivername> . Some drivers need additional dependencies. Here are the drivers used for testing data_check:","title":"Databases"},{"location":"install/#postgresql","text":"psycopg2-binary should work on most systems without any additional dependencies. You can use data-check[postgres] to install data_check directly with psycopg2-binary : e.g. with pipx: pipx install data-check[postgres]","title":"PostgreSQL"},{"location":"install/#mysqlmariadb","text":"PyMySQL as described in https://pypi.org/project/PyMySQL/ with additional cryptography dependencies. Use pipx install data-check[mysql] to install data_check with PyMySQL[rsa] .","title":"MySQL/MariaDB"},{"location":"install/#microsoft-sql-server","text":"pyodbc needs unixodbc and the development package (unixodbc-dev) on Linux. Additionally you must install the Microsoft ODBC driver for SQL Server on your system. Use pipx install data-check[mssql] to install data_check with pyodbc .","title":"Microsoft SQL Server"},{"location":"install/#oracle","text":"cx_Oracle needs Oracle client libraries to work. https://cx-oracle.readthedocs.io/en/latest/user_guide/installation.html shows how to install them. Use pipx install data-check[oracle] to install data_check with cx_Oracle .","title":"Oracle"},{"location":"usage/","text":"Usage Commands data_check - Run data_check agains the default connection in the checks folder. data_check some_folder - Run data_check agains the default connection in the some_folder folder. data_check some_folder/some_file.sql - Run data_check against the default connection for a single test. data_check -c/--connection CONNECTION - use another connection than the default. data_check -n/--workers WORKERS - use WORKERS threads to run the queries (default: 4). data_check --print - print failed results data. data_check --print-format FORMAT - format for printing failed results (pandas, csv). data_check --print-csv - shortcut for \"--print --print-format csv\". data_check -g/--gen/--generate - generate expectation files if they don't exist. data_check --force - when set, --generate will overwrite files. data_check --config CONFIG - config file to use (default: data_check.yml). data_check --ping - tries to connect to the database. data_check --verbose - print verbose output. data_check --traceback - print traceback output for debugging. data_check --version - Show the version and exit. data_check --help - show this help message and exit. Exit codes Possible exit codes: Exit code 0: All tests run successfully. Exit code 1: At least one test failed. SQL file Each SQL file must contain a single SQL query. The query be run against the database, hence you must use the SQL dialect that the database in use understands. Templates SQL files can contain Jinja2 templates. The templates are replaced with the values defined in checks/template.yml . Example: SQL file: select '{{template_value}}' as test checks/template.yml : template_value: ABC CSV file: test ABC CSV format data_check uses a pretty basic CSV format. Each column is separated by a comma without any space around them. The first line must contain a header. The columns must match the columns in the SQL file. Any columns that do not match between the CSV and the SQL file will be ignored. string_header,int_header,float_header,date_header,null_header string,42,42.1,2020-12-20, # a comment line \"second row\",42,42.1,2020-12-20, Each file starting with a '#' is regarded as a comment. You can use comments to annotate the date as they will be completely ignored. Only the data types strings and decimals are supported. Strings can be optionally enclosed in double quotes (\"). Empty strings are treated as null values. Any other data type (e.g. date) must be converted to strings/varchar in the SQL query. Generating expectation files If you run data_check --generate in a project folder, data_check will execute the query for each SQL file where the CSV file is missing and write the results into the CSV file.","title":"Usage"},{"location":"usage/#usage","text":"","title":"Usage"},{"location":"usage/#commands","text":"data_check - Run data_check agains the default connection in the checks folder. data_check some_folder - Run data_check agains the default connection in the some_folder folder. data_check some_folder/some_file.sql - Run data_check against the default connection for a single test. data_check -c/--connection CONNECTION - use another connection than the default. data_check -n/--workers WORKERS - use WORKERS threads to run the queries (default: 4). data_check --print - print failed results data. data_check --print-format FORMAT - format for printing failed results (pandas, csv). data_check --print-csv - shortcut for \"--print --print-format csv\". data_check -g/--gen/--generate - generate expectation files if they don't exist. data_check --force - when set, --generate will overwrite files. data_check --config CONFIG - config file to use (default: data_check.yml). data_check --ping - tries to connect to the database. data_check --verbose - print verbose output. data_check --traceback - print traceback output for debugging. data_check --version - Show the version and exit. data_check --help - show this help message and exit.","title":"Commands"},{"location":"usage/#exit-codes","text":"Possible exit codes: Exit code 0: All tests run successfully. Exit code 1: At least one test failed.","title":"Exit codes"},{"location":"usage/#sql-file","text":"Each SQL file must contain a single SQL query. The query be run against the database, hence you must use the SQL dialect that the database in use understands.","title":"SQL file"},{"location":"usage/#templates","text":"SQL files can contain Jinja2 templates. The templates are replaced with the values defined in checks/template.yml . Example: SQL file: select '{{template_value}}' as test checks/template.yml : template_value: ABC CSV file: test ABC","title":"Templates"},{"location":"usage/#csv-format","text":"data_check uses a pretty basic CSV format. Each column is separated by a comma without any space around them. The first line must contain a header. The columns must match the columns in the SQL file. Any columns that do not match between the CSV and the SQL file will be ignored. string_header,int_header,float_header,date_header,null_header string,42,42.1,2020-12-20, # a comment line \"second row\",42,42.1,2020-12-20, Each file starting with a '#' is regarded as a comment. You can use comments to annotate the date as they will be completely ignored. Only the data types strings and decimals are supported. Strings can be optionally enclosed in double quotes (\"). Empty strings are treated as null values. Any other data type (e.g. date) must be converted to strings/varchar in the SQL query.","title":"CSV format"},{"location":"usage/#generating-expectation-files","text":"If you run data_check --generate in a project folder, data_check will execute the query for each SQL file where the CSV file is missing and write the results into the CSV file.","title":"Generating expectation files"}]}